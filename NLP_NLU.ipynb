{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_NLU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chettkulkarni/MachineLearning/blob/master/NLP_NLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-LDjY1tiCUs",
        "colab_type": "code",
        "outputId": "ff9dc7b7-d0b5-4253-dc44-bcb0b7b26dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "import pandas as pd\n",
        "!pip install textract\n",
        "import os\n",
        "import requests\n",
        "import textract\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "import random\n",
        "random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textract\n",
            "  Downloading https://files.pythonhosted.org/packages/32/31/ef9451e6e48a1a57e337c5f20d4ef58c1a13d91560d2574c738b1320bb8d/textract-1.6.3-py3-none-any.whl\n",
            "Collecting extract-msg==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/90/84485a914ed90adb5e87df17e626be04162fbba146dfecf34643659a4633/extract_msg-0.23.1-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hCollecting xlrd==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.3MB/s \n",
            "\u001b[?25hCollecting docx2txt==0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/7d/60ee3f2b16d9bfdfa72e8599470a2c1a5b759cb113c6fe1006be28359327/docx2txt-0.8.tar.gz\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from textract) (1.12.0)\n",
            "Collecting SpeechRecognition==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 117kB/s \n",
            "\u001b[?25hCollecting EbookLib==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/38/7d6ab2e569a9165249619d73b7bc6be0e713a899a3bc2513814b6598a84c/EbookLib-0.17.1.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 36.1MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.0MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20181108\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/fd/6e8746e6965d1a7ea8e97253e3d79e625da5547e8f376f88de5d024bacb9/pdfminer.six-20181108-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 47.4MB/s \n",
            "\u001b[?25hCollecting argcomplete==1.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/82/f44c9661e479207348a979b1f6f063625d11dc4ca6256af053719bbb0124/argcomplete-1.10.0-py2.py3-none-any.whl\n",
            "Collecting python-pptx==0.6.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/86/eb979f7b0333ec769041aae36df8b9f1bd8bea5bbad44620663890dce561/python-pptx-0.6.18.tar.gz (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal==1.5.1 in /usr/local/lib/python3.6/dist-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
            "Collecting olefile==0.46\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 46.5MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/39/e1c2c2c6e2356ab6ea81fcfc0a74b044b311d6a91a45300811d9a6077ef7/IMAPClient-2.1.0-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from EbookLib==0.17.1->textract) (4.2.6)\n",
            "Collecting soupsieve>=1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/05/cf/ea245e52f55823f19992447b008bcbb7f78efc5960d77f6c34b5b45b36dd/soupsieve-2.0-py2.py3-none-any.whl\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six==20181108->textract) (2.1.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from python-pptx==0.6.18->textract) (7.0.0)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/1f/2092a81056d36c1b6651a645aa84c1f76bcee03103072d4fe1cb58501d69/XlsxWriter-1.2.8-py2.py3-none-any.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2018.9)\n",
            "Building wheels for collected packages: docx2txt, EbookLib, python-pptx, olefile\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-cp36-none-any.whl size=3963 sha256=79dcd0661ed876e9b1f770d10112accd314521b1c1a3817ed719a4be314e8298\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/1f/26/a051209bbb77fc6bcfae2bb7e01fa0ff941b82292ab084d596\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.17.1-cp36-none-any.whl size=38164 sha256=97d3c3fa97e3ba06200c4dec1f2fdb2235044071902425959b6f805c04dac399\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/11/01/951369cbbf8f96878786a1f4da68bd7ac19a5d945b38e03d54\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.18-cp36-none-any.whl size=275706 sha256=8a91058bb4b47e77b5366738eb92bf7cc2ab7b6fb0f2e93b572c0b96ecfde023\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/1f/2c/29acca422b420a0b5210bd2cd7e9669804520d602d2462f20b\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=5bffcf5fc244d33079de4fd7d0168796ca91ea0118aa94ab9d901ae84cf9a743\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built docx2txt EbookLib python-pptx olefile\n",
            "Installing collected packages: olefile, imapclient, extract-msg, xlrd, docx2txt, SpeechRecognition, EbookLib, soupsieve, beautifulsoup4, pycryptodome, pdfminer.six, argcomplete, XlsxWriter, python-pptx, textract\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.2.8 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 olefile-0.46 pdfminer.six-20181108 pycryptodome-3.9.7 python-pptx-0.6.18 soupsieve-2.0 textract-1.6.3 xlrd-1.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wPHbcIJ1FTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_10_most_common_words(count_data, count_vectorizer):\n",
        "    import matplotlib.pyplot as plt\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    total_counts = np.zeros(len(words))\n",
        "    for t in count_data:\n",
        "        total_counts+=t.toarray()[0]\n",
        "    \n",
        "    count_dict = (zip(words, total_counts))\n",
        "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
        "    words = [w[0] for w in count_dict]\n",
        "    counts = [w[1] for w in count_dict]\n",
        "    x_pos = np.arange(len(words)) \n",
        "    \n",
        "    plt.figure(2, figsize=(15, 15/1.6180))\n",
        "    plt.subplot(title='10 most common words')\n",
        "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
        "    sns.barplot(x_pos, counts, palette='husl')\n",
        "    plt.xticks(x_pos, words, rotation=90) \n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('counts')\n",
        "    plt.show()\n",
        "def print_topics(model, count_vectorizer, n_top_words):\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rieD2GKBlBkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lets_lda(url,filename):\n",
        "  myfile = requests.get(url)\n",
        "  open(filename, 'wb').write(myfile.content)\n",
        "  text = textract.process(filename)\n",
        "  text=text.decode('utf-8')\n",
        "  result = re.sub(r\"\\W\", \" \", text, flags=re.I)\n",
        "  print(result)\n",
        "  result=result.lower()\n",
        "  result=re.sub('[,\\.!?]',' ',result)\n",
        "  result=re.sub(\" \\d+\", \" \", result)\n",
        "  wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "  long_string=result\n",
        "  wordcloud.generate(long_string)\n",
        "  wordcloud.to_image()\n",
        "\n",
        "  sns.set_style('whitegrid')\n",
        "  %matplotlib inline\n",
        "\n",
        "  count_vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "  string=result.split(' ')\n",
        "  count_data = count_vectorizer.fit_transform(string)\n",
        "  plot_10_most_common_words(count_data, count_vectorizer)\n",
        "\n",
        "  number_topics = 10\n",
        "  number_words = 10\n",
        "  lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "  lda.fit(count_data)\n",
        "  print(\"Topics found via LDA:\")\n",
        "  print_topics(lda, count_vectorizer, number_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az3JtgvJksHk",
        "colab_type": "code",
        "outputId": "707ca998-c0d8-4df9-9d0e-88e901c10c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lets_lda('https://arxiv.org/pdf/2002.07526.pdf','AI.pdf')\n",
        "lets_lda('https://arxiv.org/pdf/2004.04153.pdf','Bio.pdf')\n",
        "lets_lda('https://arxiv.org/pdf/2004.04167.pdf','Phy.pdf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Survey of Deep Learning Techniques for Neural  Machine Translation  Shuoheng Yang  Yuxin Wang  Xiaowen Chu  Department of Computer science Hong Kong Baptist University  yshuoheng gmail com   yxwang  chxw  comp hkbu edu hk  Hong Kong  China  0 2 0 2     b e F 8 1           L C   s c        1 v 6 2 5 7 0     2 0 0 2   v i X r a  Abstract In recent years  natural language processing  NLP  has got great development with deep learning techniques  In the sub ﬁeld of machine translation  a new approach named Neural Machine Translation  NMT  has emerged and got massive attention from both academia and industry  However  with a signiﬁcant number of researches proposed in the past several years  there is little work in investigating the development process of this new technology trend  This literature survey traces back the origin and principal development timeline of NMT  investigates the important branches  categorizes different research orientations  and discusses some future research trends in this ﬁeld   Index Terms Neural Machine Translation  Deep Learning   Attention Mechanism   I  INTRODUCTION A  Introduction of Machine Translation  Machine translation  MT  is a classic sub ﬁeld in NLP that investigates how to use computer software to translate the text or speech from one language to another without human involvement  Since MT task has a similar objective with the ﬁnal target of NLP and AI  i e   to fully understand the human text  speech  at semantic level  it has received great attention in recent years  Besides the scientiﬁc value  MT also has huge potential of saving labor cost in many practical applications  such as scholarly communication and international business negotiation   Machine translation task has a long research history with many efﬁcient methods proposed in the past decades  Re  cently  with the development of Deep Learning  a new kind of method called Neural Machine Translation  NMT  has emerged  Compared with the conventional method like Phrase  Based Statistical Machine Translation  PBSMT   NMT takes advantages in its simple architecture and ability in capturing long dependency in the sentence  which indicates a huge potential in becoming a new trend of the mainstream  After a primitive model in origin  there are a variety of NMT models being proposed  some of which have achieved great progresses with the state of the art result  This paper summarizes the major branches and recent progresses in NMT and discusses the future trend in this ﬁeld   B  Related Work and Our Contribution  Although there is little work in the literature survey of NMT  some other works are highly related  Lipton et al  have  summarized the conventional methods in sequence learning  120   which provided essential information for the origin of NMT as well as the related base knowledge  Britz et al  and Tobias Domhan have done some model comparison work in NMT with experiment and evaluation in the practical performance of some wildly accepted technologies  but they have rare theoretical analysis  especially in presenting the relationship between different proposed models  22   118   On the other hand  some researchers limited their survey work on a special part related to NMT the Attention Mechanism  but both of them have a general scope that oriented to all kinds of AI tasks with Attention  116   117   Maybe the most related work was an earlier doctoral thesis written by Minh  Thang Luong in 2016  156   which included a comprehensive description about the original structure of NMT as well as some wildly applied tips   This paper  however  focuses on a direct and up to date literature survey about NMT  We have investigated a lot about the relevant literature in this new trend and provided compre  hensive interpretation for current mainstream technology in NMT   As for concrete components  this literature survey investi  gates the origin and recent progresses in NMT  categorizes these models by their different orientation in the model struc  ture  Then we demonstrate the insight of these NMT types  summarize the strengths and weaknesses by reviewing their design principal and corresponding performance analysis in translation quality and speed  We also give a comprehensive overview of two components in NMT development  namely attention mechanism and vocabulary coverage mechanism  both of which are indispensable for current achievement  At last  we give concentration on some literature which proposed advanced models with comparison work  we introduce these considerable models as well as the potential direction in future work   Regarding the survey scope  some subareas of NMT with less attention were deliberately left out of the scope except with brief description in future trend  These include but are not limited to the literature of robustness of NMT  domain adaptation in NMT and other applications that embed NMT method  such as speech translation  document translation   Although the research scope has been speciﬁcally designed  due to the numerous of researches and the inevitable expert   selection bias  we believe that our work is merely a snapshot of part of current research rather than all of them  We are hoping that our work could provide convenience for further research   The remaining of the paper is organized as follows  Sec  tion II provides an introduction of machine translation and presents its history of development  Section III introduces the structure of NMT and the procedure of training and testing  Section IV discusses attention mechanism  an essential innovation in the development of NMT  Section V surveys a variety of methods in handling word coverage problem and some ﬂuent divisions  Section VI describes three advanced models in NMT  Finally  Section VII discusses the future trend in this ﬁeld   II  HISTORY OF MACHINE TRANSLATION  Machine translation  MT  has a long history  the origin of this ﬁeld could be traced back to the 17th century  In 1629  Ren e Descartes came up with a universal language that expressed the same meaning in different languages and shared one symbol   The speciﬁc research of MT began at about 1950s  when the ﬁrst researcher in the ﬁeld  Yehoshua Bar Hillel  began his research at MIT  1951  and organized the ﬁrst International Conference on Machine Translation in 1952  Since then  MT has experienced three primary waves in its development  the Rule based Machine Translation  2   the Statistical Machine Translation  3   4   and the Neural Machine Translation  7   We brieﬂy review the development of these three stages in the following   A  Development of Machine Translation  1  Rule based Machine Translation  Rule based Machine Translation is the ﬁrst design in MT  which is based on the hypothesis that all different in representing the same meaning  Because in usual  a word in one language could ﬁnd its corresponding word in another language with the same meaning   languages have its symbol  In this method  the translation process could be treated as the word replacement in the source sentence  In terms of  rule based   since different languages could represent the same meaning of sentence in different word order  the word replacement method should base on the syntax rules of both two languages  Thus every word in the source sentence should take its corresponding position in the target language   The rule based method has a beautiful theory but hardly achieves satisfactory performance in implementation  This is because of the computational inefﬁciency in determining the adaptive rule of one sentence  Besides  grammar rules are also hard to be organized  since linguists summarize the grammar rules  and there are too many syntax rules in one language  especially language with more relaxed grammar rules   It is even possible that two syntax rules conﬂict with each other  The most severe drawback of rule based method is that it has ignored the need of context information in the translation process  which destroys the robustness of rule based machine  translation  One famous example was given by Marvin Minsky in 1966  where he used two sentences given below    T he pen is in the box   T he box is in the pen  Both sentences have the same syntax structure  The ﬁrst sentence is easy to understand  but the second one is more confusing  since the word  pen  is a polysemant  which also means  fence  in English  But it is difﬁcult for the computer to translate the  pen  to that meaning  the word replacement is thus an unsuccessful method   2  Statistical Machine Translation  Statistical Machine Translation  SMT  has been the mainstream technology for the past 20 years  It has been successfully applied in the industry  including Google translation  Baidu translation  etc   Different from Rule based machine translation  SMT tackles the translation task from a statistical perspective  Concretely  the SMT model ﬁnds the words  or phrases  which have the same meaning through bilingual corpus by statistics  Given one sentence  SMT divides it into several sub sentences  then every part could be replaced by target word or phrase   The most prevalent version of SMT is Phrase based SMT  PBSMT   which in general includes pre processing  sentence alignment  word alignment  phrase extraction  phrase feature preparation  and language model training  The key component of a PBSMT model is a phrase based lexicon  which pairs phrases in the source language with phrases in the target language  The lexicon is built from the training data set which is a bilingual corpus  By using phrases in this translation  the translation model could utilize the context information within phrases  Thus PBSMT could outperform the simple word to  word translation methods   3  Neural Machine Translation  It has been a long time since the ﬁrst try on MT task by neural network  44   43   Because of the poor performance in the early period and the computing hardware limitation  related research in translation by neural network has been ignored for many years   Due to the proliferation of Deep Learning in 2010  more and more NLP tasks have achieved great improvement  Using deep neural networks for MT task has received great attention as well  A successful DNN based Machine Translation  NMT  model was ﬁrst proposed by Kalchbrenner and Blunsom  8   which is a totally new concept for MT by that time  Comparing with other models  the NMT model needs less linguistic knowledge but can produce a competitive performance  Since then  many researchers have reported that NMT can perform much better than the traditional SMT model  1   112   113   114   115   and it has also been massively applied to the industrial ﬁeld  24    B  Introduction of Neural Machine Translation  1  Motivation of NMT  The inspiration for neural machine translation comes from two aspects  the success of Deep Learning in other NLP tasks as we mentioned  and the unresolved problems in the development of MT itself   For the ﬁrst reason  in many NLP tasks  traditional Machine Learning method is highly dependent on hand crafted features   that often come from linguistic intuition  which is deﬁnitely an empirical trial and error process  133   134  and is often far more incomplete in representing the nature of original data  For example  the context size of training language model is assigned by researchers with strong assumption in context relation  136   and in text representation method  the classic bag of words  BOW  method has ignored the inﬂuence of word order  135   However  when applying deep neural network  DNN  in the aforementioned tasks  the DNN requires minimum domain knowledge and avoids some pre processing steps in human feature engineering  22    DNN is a powerful neural network which has achieved excellent performance in many complex learning tasks which are traditionally considered difﬁcult  137   138   In NLP ﬁeld  DNN has been applied in some traditional tasks  for example  speech recognition  5  and Named Entity Recognition  NER   133   With the exceptional performance they got  DNN based models have found many potential applications in other NLP tasks   For the second reason  in MT ﬁeld  PBSMT has got a pretty good performance in the past decades  but there are still some inherent weaknesses which require further improvement  First  since the PBSMT generates the translation by segmenting the source sentence into several phrases and doing phrase replacement  it may ignore the long dependency beyond the length of phrases and thus cause inconsistency in translation results such as incorrect gender agreements  Second  there are generally many intricate sub components in current sys  tems  13   14   15   e g   language model  reordering model  length unknown penalties  etc  With the increasing number of these sub components  it is hard to ﬁne tune and combine each other to get a more stable result  23    All the above discussions have indicated the bottleneck in the development of SMT miniature  Speciﬁcally  this bottle  neck mainly comes from the language model  LM   This is because  in MT task  language model actually can give the most important information  the emergence probability of a particular word  or phrase  that is conditioned on previous words  So building a better LM can deﬁnitely improve the translation performance   The vast majority of conventional LM is based on the  Fig  1  The training process of RNN based NMT  The symbol   EOS   means end of sequence  The embedding layer is for pre processing  The two RNN layers are used to represent the sequence   Markov assumption   p  x1  x2          xT      T cid 89    T cid 89   t 1  p  xt x1          xt 1   p  xt xt n          xt 1    1   t 1  where x1  x2       xT is a sequence of words in a sentence and T represents the length of the sentence   In this assumption  the probability of the sentence is equal to the multiplication of probability of each word  n is the total number of words that is chosen to simplify the model  which is also referred to as context window   Obviously  the dependency of words that exceed n would be ignored  which implies that the conventional LM performs poorly on modeling long dependency  Moreover  since the experimental result has indicated that a modest context size  generally 4 6 words  can be accepted  the ﬁrst problem of traditional LM is the limited representation ability   Besides  the data sparsity for training has always been the problem that hinders an LM built with a larger size of context window  This is because the number of n tuples for counting is exponential in n  In other words  when building an LM  with the increment of the number of order  the number of training samples we need would also increase remarkably  which is also referred to as  curse of dimensionality   For example  if one LM has the order of 5 with a vocabulary size of 10 000  then the possible combination of words for statistics should be about 1025  which requires enormous training data  And since most of these combinations have not been observed before  subsequent researches have used various trade off and smoothing method to alleviate the sparsity problem  129   128   127   130   131    While further research of the aforementioned LM with statistical method has become almost stagnant  Neural Lan  guage Model  NLM   6   on the other hand  uses a neural network to build a language model that models text data directly  In initial stage  NLM used ﬁxed length of a feature vector to represent each word  and then the solid number of word vectors would concatenate together as a semantic metric to represent the context  6   38   39   which is very similar to the context window  This work was enhanced later by injecting additional context information from source sentence  12   132   126   Comparing with the traditional LM  the original NLM alleviates the sample sparsity due to the distributed representation of the word  which enables them to share the statistical weights rather than being independent variables  And since words with similar meaning may occur in the similar context  the corresponding feature vector would have the similar value  which indicates that the semantic relation of words has been  embedded  into the feature vector  New proposals in the next stage solve the long dependency problem by using Recurrent Neural Network  RNN   RNN based NLM  RNLM  models the whole sentence by reading each word once a time step  the true conditional probability without limitation of context size  41    thus it can model   Before the emergence of NMT  the RNLM  as mentioned earlier  outperformed the conventional LM in the evaluation of text perplexity and brought better performance in many practical tasks  41   26    The direct application of NLM in SMT has been naturally proposed  12   36   37   40   58   and the preliminary ex  periment indicated promising results  The potential of NLM motivates further exploration for a complete DNN based translation model  Subsequently  a more  pure  model with the only neural network has emerged  with the DNN architecture that learns to do the translation task end to end  Section III demonstrates its basic structure  in Fig  4   as well as its concrete details   2  Formulation of NMT Task  Currently  NMT task is originally designed as an end to end learning task  It directly processes a source sequence to a target sequence  The learning objective is to ﬁnd the correct target sequence given the source sequence  which can be seen as a high dimensional classiﬁcation problem that tries to map the two sentences in the semantic space  In all mainstreams of modern NMT model  this process can be divided into two steps  encoding and decoding  and thus can functionally separate the whole model as Encoder and Decoder as illustrated in Fig  2   Fig  2  End to End structure in modern NMT model  The encoder is used to represent the source sentence to semantic vector  while the decoder makes prediction from this semantic vector to a target sentence  End to End means the model processes source data to target data directly  without explicable intermediate result   In perspective of probability  NMT generates the target se  quence T  t1  t2       tm  from the max conditional probability given the source sequence S s1  s2       sn   where n is the length of sequence S and m is the length of target sequence T   The whole task could be formulated as  24    argmax P  T S     2   More concretely  when generating each word of the target sentence  it uses the information from both the word it predicted previously and the source sentence  In that case  each generating step could be described as when generating the i th word   argmax  P  ti tj i  S    3   m cid 89   i 1  Based on this formula and the discussion of NLM above  NMT task could be regarded as an NLM model with additional constraints  e g   conditioned on a given source sequence    C  The Recent Development in NMT  We devide the recent developement of NMT in ﬁve main stages   a  the original NMT with a shallow layer   b  SMT  assisted by NLM   c  the DNN based NMT   d  NMT with attention mechanism   e  the attention based NMT   NMT with Shallow Layer Even before the Deep Learning  Allen has used binary encoding to train an NMT model in 1987  44   Later in 1991  Chrisman used Dual ported RAAM architecture  42  to build an original NMT model  43   Although both of them have a pretty primitive design with the limited result when looking back  their work has indicated the original idea of this ﬁeld  The further related work has almost stagnated in the following decades  due to the huge progress that SMT method acquired at that period  as well as the limited computing power and data samples   SMT assisted by NLM Based on the above discussion  NLM has revolutionized the traditional LM even before the rise of deep learning  Later on  deep RNN based NLM has been applied in the SMT system  Cho et al  proposed an SMT model along with an NLM model  18   Although the main body is still SMT  this hybrid method provides a new direction for the emergence of a pure deep learning based NMT   NMT with Deep Neural Network Since the traditional SMT model with NLM has got the state of the art performance at that time  a pure DNN based translation approach was proposed later with an end to end design to model the entire MT process  8   16   Using DNN based NMT could capture subtle irregularities in both two languages more efﬁciently  24   which is similar to the ob  servation that DNNs often have a better performance than  shallow  neural networks  21    NMT with Attention Mechanism Although the initial DNN based NMT model has not outperformed the SMT completely  it still exhibited a huge potential for further research  When tracing back the major weakness  although one theoretical advantage of RNN is its ability in capturing the long dependency between words  in fact  the model performance would deteriorate with the increase of sentence length  This scenario is due to the limited feature representation ability in a ﬁxed length vector  Under the circumstances  since the original NMT has got a pretty good performance without any auxiliary  the idea of whether some variants in architecture could bring a breakthrough has led to the rise of Attention Mechanism   Attention Mechanism was originally proposed by Bahdanau et al  as an intermediate component  21   and the objective is to provide additional word alignment information in translating the long sentence  Surprisingly  NMT model has got a con  siderable improvement with the help of this simple method  Later on  with tremendous popularity among both academia and industry  many reﬁnements in Attention Mechanism have emerged  and more details will be discussed in Section IV   Fully Attention based NMT With the development of Attention Mechanism   fully Attention based NMT has emerged as a great innovation in NMT history  In this new tendency  Attention mechanism has taken the dominate position in text feature extraction rather   than a auxiliary component  And the representative model is Transformer  25   which is a fully attention based model proposed by Vaswani et al   Abandoning previous framework in neither RNN nor CNN based NMT models  Transformer is a that solely based on an intensiﬁed version of Attention Mechanism called Self  Attention with feed forward connection  which got revolu  tionary progress in structure with state of the art performance  Speciﬁcally  the innovative attention structure is the secret sauce to gain such signiﬁcant improvement  The self attention is a powerful feature extractor which also allows to  read  the entire sentence and model it once a time  In the perspective of model architecture  this character can be seen as a combination of advantages from both CNN and RNN  which endows it a good feature representation ability with high inference speed  More details about self attention will be given in Section IV  The architecture of Transformer will be discussed in Section VI   III  DNN BASED NMT  The emergence of DNN based NLM indicates the feasibility of building a pure DNN based translation model  The further implementation is the def acto form of NMT in origin  This section reviews the basic concept of DNN based NMT  demon  strates a comprehensive introduction of the standard structure of the original DNN based NMT  and discusses the training and inferencing processes   A  Model Design of DNN based NMT  There are many variations of network design for NMT  which can be categorized into recurrent or non  recurrent models  More speciﬁcally  this category can be traced back to the early development of NMT  when RNN and CNN based models are the most common design  Many sophisticated models proposed afterwards also belong to either CNN or RNN family  This sub section follows the development of NMT in the early years  and demonstrates some representative models by classifying them as RNN or CNN based models  1  RNN based NMT  Although in theory  any network with enough feature extraction ability could be selected to build an NMT model  in def acto implementations  RNN based NMT models have taken the dominant position in NMT development  and they have achieved state of the art performance  Based on the discussion in Section II  since many NLM literature used RNN to model the sequence data  this design has intuitively motivated the further work to build an RNN based NMT model  In the initial experiment  an RNN based NLM was applied as a feature extractor to compress the source sentence into a feature vector  which is also referred to as thought vector  Then a similar RNN was applied to do the  inverse work  to ﬁnd the target sentence that can match the previous thought vector in semantic space   The ﬁrst successful RNN based NMT was proposed by Sutskever et al   who used a pure deep RNN model and got a performance that approximates the best result achieved by SMT  16   Further development proposed the Attention  Mechanism  which improves the translation performance sig  niﬁcantly and exceeds the best SMT model  GNMT model was an industry level model applied in Google Translation  and it was regarded as a milestone in RNN based NMT   Besides the above mentioned work  other researchers have also proposed different architectures with excellent perfor  mance  Zhang et al  proposed Variational NMT method  which has an innovative perspective in modeling translation task  and the corresponding experiment has indicated a better performance than the baseline of original NMT in Chinese  English and English German tasks  89   Zhou et al  have designed Fast Forward Connections for RNN  LSTM   which can allow a deeper network in implementation and thus gets a better performance  88   Shazeer et al  incorporated Mixture  of Expert  MoE  architecture into the GNMT model  which has outperformed the original GNMT model  90   Concretely  MoE is one layer in the NMT model  which contains many sparsely combined experts  which are feed forward neural networks in this experiment  and is connected with the RNN layer by a gate function  This method requires more parameters in total for the NMT model  but still maintains the efﬁciency in training speed  Since more parameters often imply a better representation ability  it demonstrates huge potential in the future   2  CNN based NMT  Related work in trying other DNN models have also been proposed  Perhaps the most noted one is the Convolutional Neural Network  CNN  based NMT  In fact  CNN based models have also undergone many variations in its concrete architecture  But for a long while  most of these models can t have competitive performance with RNN based model  especially when the Attention Mechanism has emerged  In the development of CNN based NMT models  Kalch  brenner   Blunsom once tried a CNN encoder with RNN Decoder  8   and it s maybe the earliest NMT architecture applied with CNN  Cho et al  tried a gated recursive CNN encoder with RNN decoder  but it has shown worse perfor  mance than RNN encoder  18   A fully CNN based NMT was proposed by Kaiser   Bengio later  86   which applied Extended Neural GPU  119   The best performance in the early period of CNN based NMT was achieved by Gehring et al   which was a CNN encoder NMT and got the similar translation performance with RNN based model at that time  19   Concurrently  Kalchbrenner et al  also proposed ByteNet  a kind of CNN  based NMT  which achieved the state of  the art performance on character level translation but failed at word level translation  84   In addition  Meng et al  and Tu et al  proposed a CNN based model separately  which provides additional alignment information for SMT  20   83    Compared with RNN based NMT  CNN based models have its advantage in training speed  this is due to the intrinsic structure of CNN which allows parallel computations for its different ﬁlters when handling the input data  And also  the model structure has made CNN based models easier to resolve the gradient vanishing problem  However  there are two fatal drawbacks that affect their translation quality  First  since the original CNN based model can only capture the word depen    dencies within the width of its ﬁlters  the long dependency of words can only be found in high level convolution layers  this unnatural character often causes a worse performance than the RNN based model  Second  since the original NMT model compresses a sentence into a ﬁxed size of the vector  a large performance reduction would happen when the sentence becomes too long  This comes from the limited representation ability in ﬁxed size of the vector  Similar phenomenon can also be found in early proposed RNN based models  which are later alleviated by Attention Mechanism   Some advanced CNN based NMT models have also been proposed with corresponding solutions in addressing the above drawbacks  Kaiser et al  proposed the Depthwise separable convolutions based NMT  The SliceNet they created can get similar performance with Kaiser et al   2016   85   Gehring et al   2017  followed their previous work by proposing a CNN based NMT that is cooperated with Attention Mechanism  It even got a better result than RNN based model  82   but this achievement was soon outperformed by Transformer  25    B  Encoder Decoder Structure  As is known  Encoder Decoder is the most original and classic structure of NMT  it was directly inspired by NLM and proposed by Kalchbrenner   Blunsom  8  and Cho et al   18   Despite all kinds of reﬁnements in details and small tips  it was wildly accepted by almost all modern NMT models  Based on the discussion above  since RNN based NMT has held the dominant position in NMT  and to avoid being overwhelmed in describing all kinds of small distinctions between models  structures  we speciﬁcally focus our discussion just on the vanilla RNN based NMT  thus can help to trace back the development process of NMT   The original structure of Encoder Decoder structure is con  ceptually simple  It contains two connected networks  the encoder and the decoder  in its architecture  each for a different part of the translation process  When the encoder network receives one source sentence  it reads the source sentence word by word and compresses the variable length sequence into a ﬁxed length vector in each hidden state  This process is called encoding  Then given the ﬁnal hidden state of the encoder  referred to as thought vector   the decoder does the reverse work by transforming the thought vector to the target sentence word by word  Because Encoder Decoder structure addresses the translation task from source data directly to the target result  which means there s no visible result in the middle process  this is also called end to end translation  The principle of Encoder Decoder structure of NMT can be seen as mapping the source sentence with the target sentence via an intermediate vector in semantic space  This intermediate vector actually can represent the same semantic meaning in both two languages   For speciﬁc details of this structure  besides the model selection in the network  RNN based NMT models also differ in three main terms   a  the directionality   b  the type of activation function  and  c  the depth of RNN layer  156   In the following  we give a detailed description   Depth  For the depth of RNN  as we discussed in Section II  single layer RNN usually performs poorly comparing with multi layer RNN  In recent years  almost all the models with competitive performance are using a deep network  which has indicated a trend of using a deeper model to get the state  of the art result  For example  Bahdanau et al   21  used four layers RNN in their model   However  simply increasing more layers of RNN may not always be useful  In the proof proposed by Britz et al   22   they found that using 4 layers RNN in the encoder for speciﬁc dataset would produce the best performance when there is no other auxiliary method in the whole model  Besides that  stacking RNN layers may make the network become too slow and difﬁcult to train  One major challenges is the gradient exploding and vanishing problem  28   which will cause the gradient be ampliﬁed or diminished when processing back propagation in deep layers  Besides the additional gate struc  ture in reﬁned RNN  like LSTM and GRU   other methods have also been applied to alleviate this phenomenon  For example  in Wu et al  s work  the residual connections are provided between layer  which can improve the value of gradient ﬂow in the backward pass  thus can speed up the convergence process  24   Another possible problem is that a deeper model often indicates larger model capacity  which may perform worse on comparatively less training data due to the over ﬁtting   Directionality  In respect of directionality  a simple uni  directional RNN has been chosen by some researchers  For example  Luong et al  have directly used unidirectional RNN to accept the input sentence  23   In comparison  bidirec  tional RNN is another common choice that can empower the translation quality  This is because the model performance is affected by whether it  knows  well about the information in context word when predicting current word  A bidirectional RNN obviously could strengthen this ability   layer reads the sentence  left  In practice  both Bahdanau et al  and Wu et al  used bidirectional RNN on the bottom layer as an alternative to capture the context information  21   24   In this structure  the ﬁrst to right   and the second layer reads the sentence in a reverse direction  Then they are concatenated and fed to the next layer  This method generally has a better performance in experiment  although the explanation is intuitive  Based on the discussion of LM in Section II  the emergence probability of a speciﬁc word is determined by all the other words in both the prior and the post positions  When applying unidirectional RNN  word dependency between the ﬁrst word and the last word is hard to be captured by the thought vector  since the model has experienced too many states in all time steps  On the country  bidirectional RNN provides an additional layer of information with reverse direction of reading words  which could naturally reduce this relative length within steps   The most visible drawback of this method is that  it s hard to be paralleled  considering the time consuming in its realization  both Bahdanau et al  and Wu et al  choose to apply just one layer bidirectional RNN in the bottom layer   of the encoder  and other layers are all unidirectional layers  24   21   This choice makes a trade off between the feature representation ability with model efﬁciency  due to it can still enable the model to be distributed on multi GPUs  24   The basic concept of bidirectional RNN could ﬁnd in Fig  2   Activation Function Selection  In respect of activation function selection  there are three common choices  vanilla RNN  Long Short Term Memory  LSTM   17   and Gated Recurrent Unit  GRU   18   Comparing with the vanilla RNN  both the last two have some robustness in addessing the gradient exploding and vanishing problem  27   28   Another sequence processing task has also indicated better performance achieved by GRU and LSTM  26   Besides  some innovative neural units have been proposed  Wang et al  proposed linear associative units  which can alleviate the gradient diffusion phenomenon in non linear recurrent activation  92   More recently  Zhang et al  have created addition subtraction twin  gated recurrent network  ATR   This type of unit reduces the inefﬁciency in NMT training and inference by simplifying the weight matrices among units  91   All in all  in NMT task  LSTM is the most common choice   C  Training method  Before feeding the training data to the model  one pre step is to transfer the words to vectors  which makes a proper form that the neural network could receive  Usually  the most frequent V words in one language will be chosen  and each language generally has different word set  Despite that the embedding weights will be learned in the training period  the pre trained word embedding vector such as word2vec  9   10  or Glove vector  11  can also be applied directly   In the training period  this model is fed by a bilingual corpus for Encoder and Decoder  The learning objective is to map the input sequence with the corresponding sequence in the target language correctly  Like other DNN models  the input sentence pair is embedded as a list of word vectors  and the model  Fig  3  The concept of Bidirectional RNN  Fig  4  The process of greedy decoding  each time the model would predict the word with highest probability  and use the current result as the input in next time step to get further prediction  parameters are initialized randomly  The training process could be formulated as trying to updating its parameters periodically until getting the minimum loss of the neural network  In the implementation  RNN will reﬁne the parameters after it processes a subset of data that contain a batch of training samples  this subset is called the mini batch set  To simplify the discussion of the training process  we take one sentence pair  one training sample  as example   For the Encoder  the encoding RNN will receive one word in source sentence once a time step  After several steps  all words will be compressed into the hidden state of the Encoder  Then the ﬁnal vector will be transferred to the Decoder   For Decoder  the input comes from two sources  the thought vector that is directly sent to Decoder  and the correct word in the last time step  the ﬁrst word is   EOS     The output process in Decoder can be seen as a reverse work of Encoder  Decoder predicts one word in each time step until the last symbol is   EOS     D  Inference method  After the training period   the model could be used for translation  which is called inference  The inference procedure is quite similar to the training process  Nevertheless  there is still a clear distinction between training and inference  at decoding time  we only have access to the source sentence  i e   encoder hidden state   There is more than one way to perform decoding  Proposed decoding strategies include Sampling and Greedy search  while the latter one is generally accepted and be evolved as Beam search   1  General decoding work ﬂow  greedy   The idea of greedy strategy is simple  as we illustrate in Fig  4  The Greedy strategy is only considering the predicted word with the high  est probability  In the implementation of our illustration  the previously generated word would also be fed to the network together with the thought vector in the next time step  The detailed steps are as follows   1  The model still encodes the source sentence in the same way as during the training period to obtain the thought vector  and this thought vector is used to initialize the decoder    2  The decoding  translation  process will start as soon as the decoder receives the end of sentence marker   EOS   of source sentence   3  For each time step on the decoder side  we treat the RNN s output as a set of logits  We choose the word with the highest translation probability as the emitted word  whose ID is associated with the maximum logit value  For example  in Fig  4  the word  moi  has the highest probability in the ﬁrst decoding step  We then feed this word as an input in the next time step  The probability is thus conditioned on the previous prediction  this is why we call it  greedy  behavior    4  The process will continue until the ending symbol    EOS   is generated as an output symbol   2  Beam search  While the Greedy search method has produced a pretty good result  Beam search is a more elab  orated one with better results  Although it is not a necessary component for NMT  Beam search has been chosen by most of NMT models to get the best performance  22    The beam search method was proposed by other sequence learning task with successful application  29   30   It s also the conventional technique of MT task that has been used for years in ﬁnding the most appropriate translation result  34   32   33   Beam search can be simply described as retaining the top k possible translations as candidates at each time  where k is called the beam width  In the next time step  each candidate word would be combined with a new word to form new possible translation  The new candidate translation would then compete with each other in log probability to get the new top k most reasonable results  The whole process continues until the end of translation   Concretely   the beam search can be formulated in the  following steps   Algorithm 1 Beam Search  set Beamsize   K  h0   encoder S  t   1    LS means length of source sentence     α is Length factor  while n   α   LS do y1 i     EOS   while i   K do  set ht   decoder ht 1  yt i   set Pt i   Sof tmax yt i   set yt 1 i   argT op K Pt i   set i   i   1  end while set i   0 if ht      EOS   then  break   end if set t   t   1  end while select argmax p Y    from K candidates Yi return Yi  Besides the standard Beam search which ﬁnds the candidate translation only by sorting log probability  this evaluation function mathematically tends to ﬁnd shorter sentence  This is because a negative log probability would be added at each decoding step  which lowers the scores with the increasing length of sentences  31   An efﬁcient variant for alleviating this scenario is to add a length normalization  7   A reﬁned length normalization was also proposed by Wu et al   24    Another kind of reﬁned method in Beam search is adding coverage penalty  which helps to encourage the decoder to cover the words in the source sentence as much as possible when generating an output sentence  24   35    In addition  since this method ﬁnds k times of transla  tion rather than one  until getting the ﬁnal result  it generally makes the decoding process more time consuming  In practice  an intuitive solution is to limit the beam width as a small constant  which is a trade off between the decoding efﬁciency and the translation accuracy  As reported by a comparison work  an experimental beam width for best performance is 5 to 10  22    IV  NMT WITH ATTENTION MECHANISM  A  Motivation of Attention Mechanism  While the promising performance of NMT has indicated its great potential in capturing the dependencies inside the sequence  in practice  NMT still suffers a huge performance reduction when the source sentence becomes too long  Com  paring with other feature extractors  the major weakness of the original NMT Encoder is that it has to compress one sentence into a ﬁxed length vector  When the input sentence becomes longer  the performance deteriorates because the ﬁnal output of the network is a ﬁxed length vector  which may have limitation in representing the whole sentence and cause some information loss  And because of the limited length of vector  this information loss usually covers the long range dependencies of words  While increasing the dimension of encoding vector is an intuitive solution  since the RNN training speed is naturally slow  a larger vector size would cause an even worse situation   Attention Mechanism emerged under this circumstance  Bahdanau et al   21  initially used this method as a supplement that can provide additional word alignment information in the decoding process  thus can alleviate the information reduction when the input sentence is too long  Concretely  Attention Mechanism is an intermediate component between Encoder and Decoder  which can help to determine the word correlation  word alignment information  dynamically  In the encoding period  it extends the vector of the ﬁnal state in the original NMT model with a weighted average of hidden state in each time state  and a score function is provided to get the weight we mention above by calculating the correlation of each word in source sentence with the current predicting word  Thus the decoder could adapt its concentration in different translation steps by ordering the importance of each word correlation in source sentence  and this method can help to capture the long  range dependencies for each word respectively    The inspiration for applying the Attention Mechanism on NMT comes from human behavior in reading and translating the text data  People generally read text repeatedly for mining the dependency within the sentence  which means each word has different dependency weight with each other  Comparing with other models in capturing word dependency information such as pooling layer in CNN or N gram language model  attention mechanism has a global scope  When ﬁnding the dependency in one sequence  N gram model will ﬁx its the searching scope in a small range  usually the N is equal to 2 or 3 in practice  Attention Mechanism  on the other hand  calculates the dependency between the current generating word with other words in source sentence  This more ﬂexible method obviously bring a better result   The practical application of Attention Mechanism is actually far beyond the NMT ﬁeld  and it is even not an invention in NMT development  Some other tasks have also proposed similar methods that give weighted concentration on different position of input data  for example  Xu et al  109   proposed similar mechanism in handling image caption task  which can helps to dynamically locate different entries in image feature vector when generating description of them  Due to the scope of this survey  the following discussion would only focus on the Attention Mechanism in NMT   B  Structure of Attention Mechanism  There are many variants in the implementation of Attention Mechanism  Here we just give the detailed description of Attention Mechanism which has been widely accepted as bringing signiﬁcant contribution in the development of NMT  1  basic structure  The structure of attention mechanism was originally proposed by Bahdanau et al  In later  Luong et al  proposed similar structure with small distinctions and extends this work  23   21    To simplify the discussion  here we take Luong et al  s method as an example  Concretely  in encoding period  this mechanism receiving the input words like the basic NMT model  but instead of compressing all the information in one vector  every unit in the top layer of encoder will generate one vector that represents one time step in the source sentence   Fig  5  The concept of Attention Mechanism which can provide additional alignment information rather than just using information in ﬁxed length of vector  In the decoding period  the decoder wont predict the word just use its own information  However  it collaborates with the attention layer to get the translation  The input of attention  mechanism is the hidden states in the top layer of the en  coder and the current decoder  It gets the relativity order by calculating the following steps   1  The current decoding hidden state ht will be used to compare with all source states hs to derive the attention weights score st   2  The attention weights at  is driven by normalization  operation for all attention weight score   3  Based on the attention weights  we then compute the weighted average of the source states as a context vector ct  4  Concatenate the context vector with the current decod  ing hidden state to yield the ﬁnal attention vector the exact combination method can be different    5  The attention vector is fed as an input to decoder in the next time step  applicable for input feeding   The ﬁrst three steps can be summarized by the equations below   st   score ht  hs   exp st   exp st   at    S cid 80   cid 88   s 0  ct    aths   Attention f unction    Attention weight    Context vector    4    5    6   s  Among the above function  The score function could be  deﬁned in different ways  Here  we two classic deﬁnitions   Luong cid 48 s version   Bahdanau cid 48 s version   hT t W hs vT a tanh W1ht  hs   score ht  hs      cid 40    7  Back to the decoding period  it receives the information from both two sides  the decoder hidden state and the attention vector  given the current two vectors  it then predicts the words by alignment them to a new vector  then it usually has another layer to predict the current target word   2  Global Attention   Local Attention  Global Attention Global Attention is the method of Attention Mechanism we mentioned above  and its also a ﬂuent type in various of Attention mechanism  The idea of Global Attention is also the original form of attention mechanism  though it got this name by Luong et al   23   the corresponding term is Local Attention  The term global derives from it calculates the context vector by considering the relevance order of all words in the source sentence  This method has excellent performance because more alignment information will generally produce a better result  A straightforward presentation in Fig  6  As we have introduced in Section IV  this method considers all the source word in the decoding period  The main drawback is calculation speed deteriorates when the sequence is very long since one hidden state will be generated in one time step in the Encoder  the cost of score function would be linear with the number of time steps on the Encoder  When the input is a long sequence like a compound sentence or a paragraph  it may affect the decoding speed   Local attention Local attention was ﬁrst proposed by Luong et al   23   As illustrated in Fig  7  this model  on the   other hand  will just calculate the relevance with a subset of the source sentence  Comparing with Global attention  it ﬁxes the length of attention vector by giving a scope number  thus avoiding the expensive computation in getting context vectors  The experiment result indicated that local attention can keep a balance between model performance with computing speed  The inspiration of local attention comes from the soft attention and hard attention in image caption generation task  which was proposed by Xu et al   109   While the global attention is very similar to soft attention  the local attention  on the other hand  can be seen as a mixture method of soft attention with hard attention   In theory  although covering more information would gen  erally get a better result  the fantastic result of this method has indicated a comparable performance with global attention when it has been ﬁne tuned  This seems due to the common phenomenon in human language the current word would naturally have a high dependency with some of its nearby words  which is quite similar to the assumption of the n gram language model   In the details of calculation process  given the current target words position pt  the model ﬁxes the context vector in scope D  The context vector ct is then derived as a weighted average over the set of source hidden states within the range  pt   D  pt   D   Scope D is selected by experience  and then it could be same steps in deriving the attention vector like Global attention   3  Input feeding approach  Input feeding is a small tip in constructing the NMT structure  but from the perspective of providing alignment information  it can also be seen as a kind of attention   The concept of input feeding is simple  In the decoding period  besides using the previously predicted words as input  it also uses the attention vector that in the previous time step as additional input in next time step  1   23   This attention vectors will concatenate with input vector to get the ﬁnal input vector  then this new vector will be fed as the input in the next step   4  Attention Mechanism in GNMT   GNMT is short for Google Neural Machine Translation  which is a well known version of NMT with Attention Mechanism  GNMT was proposed by Wu et al    24  and famous for its successful application in industrial level NMT system  With the help of many kinds of advanced tips in model detail  it got state of the   Fig  6  The concept of Global attention  current decoder hidden state calculated with all the hidden states in source side to get the alignment information   Fig  7  The concept of Local attention  current hidden state calculated with a subset of all the hidden states in source side   art performance at that time  Besides  the elaborate architecture of GNMT makes it have a better inference speed  which helps it more applicable in satisfying the industry need   The concept of GNMT get the help of the current research in attention mechanism  it used Global Attention but was recon  struct by a more effective structure for model parallelization  The concrete details illustrated in the ﬁgure  it has two main points in this architecture  First  this structure has canceled the  Fig  8  Attention in GNMT  the Attention weight was driven by the bottom layer of Decoder and sent to all Decoder layers  which helps to improve computing parallelization  connection between the encoder and the decoder  So that it can have more freedom in choosing the structure of the encoder and decoder  for example  the encoder could choose the different dimensions in each layer regardless of the dimension in the decoder  only the top layer of the both encoder and decoder should have same dimensions to guarantee that they can be calculated in mathematics for driving attention vector  Second  this structure makes it easier for paralleling the model  Only the bottom layer of the decoder is used to get the context vector  then all of the remain decoding layers will use this context vector directly  This architecture can retain as much parallelism as possible   For details of attention calculation  GNMT applying the At  tention Mechanism like the way of calculating global attention  while the score   function is a feed forward network with 1 hidden layer   5  Self attention  Self attention is  also called intra  attention  it is wildly known for its application in NMT task due to the emergence of Transformer  While other commonly   More concretely  the above methods can be seen as a kind of inheritance from the alignment model in SMT research  with more experiential assumption and intuition in linguistics  P osition Bias   It assumed words in source and target sentence with the same meaning would also have a similar relative position  especially when both two sentence have a similar word order  As an adjustment of the original attention mechanism  it helps to improve the alignment accuracy by en  couraging words in similar relative position to be aligned  Fig  ure11111 demonstrated the phenomena strongly  where words in diagonal are tended to be aligned  M arkov Condition   Empirically  in one sentence  one word has higher Correlation with its nearby words rather than those far from it  this is also the basement in explaining context capture of n gram LM  As for translation task  it s obvious that words are adjacent in source sentence would also map to the nearby position in target sentence  taking advantage of this property  this consideration thus improves the alignment accuracy by discouraging the huge jumps in ﬁnding the corresponding alignment of nearby words  In addition  the method with similar consideration but different implementation is local attention  F ertility   Fertility measures whether the word has been attended at the right level  it considers preventing both scenarios when the word hasn t got enough attention or has been paid too much attention  This design comes from the fact that the poor translation result is commonly due to repeatedly translated some word or lack coverage of other words  which refers to Under translation and Over translation  Bilingual Symmetry   In theory  word alignment should be a reversible result  which means the same word alignment should be got when translation processing form A to B with translation from B to A  This motivates the parallel in both directions and encouraging the similar alignment result   training for the model  The reﬁnement infertility was further extended by Tu et al   35   who proposed fertility prediction as a normalizer before decoding  this method adjusts the context vector in original NMT model by adding coverage information when calculating attention weights  thus can provide complementary information about the probability of source words have been translated in prior steps   Besides the intuition that heuristics from SMT  Cheng et al  applied the agreement based learning method on NMT task  which encourages joint training in the agreement of word alignment with both translation directions  96   In later  Mi et al  proposed a supervised method for attention component  and it utilized annotated data with additional alignment constraints in its objective function  experiments in Chinese to English task has proven to beneﬁt for both translation performance and alignment accuracy  97    V  VOCABULARY COVERAGE MECHANISM  Besides the long dependency problem in general MT tasks  the existence of unknown words is another problem that can severely affect the translation quality  Different from traditional SMT methods which support enormous vocabulary  most of NMT models suffer from the vocabulary coverage  Fig  9  The concept of Multi head Self attention   cid 19    cid 18  QK T   dk  noted Attention Mechanism driven the context information by calculating words dependency between source sequence with target sequence  Self attention calculates the words de  pendency inside the sequence  and thus get an attention based sequence representation   As for calculation steps  Self attention ﬁrst gets 3 vectors based original embedding for different purpose  the 3 vectors are Query vector  Key vector  and Value vector  Then the attention weights was calculated in this way   Attention  Q  K  V     softmax  V   8   1  dk  is a scaled factor for avoiding to have more where the stable gradients that caused by dot products operation  In addition  the above calculation can be implemented in metrics multiplication  so the words dependency can easily got in form of relation metrics   C  Other related work  Besides the above description in signiﬁcant progress  there are also some other reﬁnements in a different perspective of attention mechanism   In perspective of attention structure  Yang et al  improved the traditional attention structure by providing a network to model the relationship of word with its previous and subsequent attention  94   Feng et al  proposed a recurrent attention mechanism to improve the alignment accuracy  and it has been proved to outperformed vanilla models in large  scale ChineseEnglish task  93    Moreover  other researches focus on the training process  Cohn et al  extended the original attention structure by adding several structural biases  they including positional bias  Markov conditioning  fertility  and Bilingual Symmetry  95   model that integrated with these reﬁnements have got better translation performance over the basic attention based model    problem due to the nature that it can only choose candidate words in predeﬁned vocabulary with a modest size  In terms of vocabulary building  the chosen words are usually frequent words  while the remaining words are called unknown words or out of vocabulary  OOV  words   Empirically speaking  the vocabulary size in NMT varies between 30k 80k at most in each language  with one marked exception was proposed by Jean et al   who once used an efﬁcient approximation for sof tmax to accommodate for the immense size of vocabulary  500k   47   However  the vocabulary coverage problem still persists widely because of the far more number of OOV words in de f acto translation task  such as proper nouns in different domains and a great number of rarely used verbs   Since the vocabulary coverage in NMT is extremely lim  ited  handling the OOV words is another research hot spot  This section demonstrates the intrinsic interpretation of the vocabulary coverage problem in NMT and the corresponding solutions proposed in the past several years   A  Description of Vocabulary Coverage problem in NMT  Based on the scenario as mentioned above  in the prac  tical implementation of NMT  the initial way is choosing a small set of vocabulary and converting a large number of OOV words to one uniform  UNK  symbol  or other tags  as illustrated in Fig  10  This intuitive solution may hurt translation performance in the following two aspects  First  the existence of  UNK  symbol in translation may hurt the semantic completeness of sentence  ambiguity may emerge when  UNK  replace some crucial words  48   Second  as the NMT model hard to learn information from OOV words  the prediction quality beyond the OOV words may also be affected  49    BLEU PERFORMANCE OF NMT MODELS  TABLE I  Model  ByteNet Deep Att   PosUnk GNMT   RL ConvS2S MoE Deep Att   PosUnk Ensemble GNMT   RL Ensemble ConvS2S Ensemble Transformer  base model  Transformer  big   BLEU  EN DE 23 75  24 6 25 16 26 03  26 3 26 36 27 3 28 4  EN FR  39 2 39 92 40 46 40 56 40 4 41 16 41 29 38 1 41 8  Besides the unsurprising observation that NMT performed poorly on sentence with more OOV words than with more frequent words  some other phenomena in MT task are also hard to be handled like multi word alignment  transliteration  and spelling   etc   16   21   They are seen as a similar phenomenon which is also caused by unknown words problem or suffers from rare training data  50    Fig  10  An example of OOV words problem presented in  23   en and f r denote the source sentence in English and the corresponding target sentence in French  nn denotes the neural network s result   For most of NMT model  choosing a modest size of vocabulary list is virtually a trade off between the computation cost with translation quality  Also  it has been found the same thing in training NLM  53   52   Concretely  the computation cost mainly comes from the nature of the method in getting predicting word a normalization operation  which is used repeatedly in training of the DL model  Speciﬁcally  in NMT task  since DL model needs to adjust the parameters each time  the probability of current word thus would be calculated repeatedly to get the gradient  and since NMT model calculates the probability of current word when making a prediction  it needs to normalize the all of words in the vocabulary each time  Unfortunately  the normalization process is time  consuming due to its time complexity is linear with the vocabulary size  and this attribute has rendered the same time complexity in the training process   B  Different Solutions  Related researches have proposed various methods in both the training and inference process  Further  these methods can be roughly divided into three categories based on their different orientations  The ﬁrst one is intuitively focused on ﬁnding solutions in improving computation speedup  which could support a more extensive vocabulary  The second one focus on using context information  this kind of method can address some of the unknown words such as Proper Noun  by copying them to translation result as well as low frequency words which cause a poor translation quality  The last one  which is more advanced  prefers to utilize information inside the word such as characters  because of their ﬂexibility in handling morphological variants of words  this method can support translating OOV words in a more  intelligent  way  1  methods by computation speedup  For computation speedup method  there are lots of literature that implement their idea in NLM training  The ﬁrst thought in trying com  putation speedup is to scale the sof tmax operation  Since an effective sof tmax calculation could obviously support a larger vocabulary  this kind of trying has got a lot of attention in NLM literature  Morin   Bengio  53  proposed hierarchical models to get an exponential speedup in the computation of normalization factor  thus help to accelerate the gradient calculation of word probabilities  In concrete details  the original model has transformed vocabulary into a binary tree structure  which was built with pre knowledge from WordNet  54    The initial experiment result shows that this hierarchical method is comparable with traditional trigram LM but fails   to exceed original NLM  this is partly because of utilizing handcrafted feature from WordNet in the tree building process  As a binary tree can provide a signiﬁcant improvement in cost effective between the speed with performance  further work still focuses on this trend to ﬁnd better reﬁnement  Later on  Mnih   Hinton followed this work by removing the requirement of expert knowledge in tree building process  52    A more elegant method is to retain the original model but change the method in calculating the normalization factor  Bengio   Senecal proposed importance sampling method to approximate the normalization factor  55   However  this method is not stable unless with a careful control  56   Mnih   Teh used noise contrastive estimation to learn the normalization factor directly  which can be more stable in the training process of NLM  57   Later  Vaswani et al  proposed a similar method with application in MT  58    The above methods are difﬁcult to be implemented par  allelly by GPUs  Further consideration found solutions that are more GPU friendly  Jean et al  alleviated the computation time by utilizing a subset of vocabulary as candidate words list in the training process while used the whole vocabulary in the inference process  Based on the inspiration of using importance sampling in earlier work  56   they proposed a pure data segmentation method in the training process  Speciﬁcally  they pre processed the training data sequentially  choosing a subset of vocabulary for each training example with the number of its distinct words reached threshold t which is still far less than the size of original vocabulary   In the inference process  they still abandon using the whole vocabulary and proposing a hybrid candidate list alternatively  They composed candidate words list from two parts  The ﬁrst part is some spe  ciﬁc candidate target words that translated from a pre deﬁned dictionary with the others are the K most frequent words  In the practical performance analysis  this method remains the similar modest size of candidate words in the training process  thus  it can maintain the computational efﬁciency while supporting an extremely larger size of candidate words  47   Similarly  Mi et al  proposed vocabulary manipulation method which provides a separate vocabulary for different sentences or batches  it contains candidate words from both word to word dictionary and phrase to phrase library  104    Besides all kinds of corresponding drawbacks in the above method  the common weakness of all these methods is they still suffer from the OOV words despite a larger vocabulary size they can support  This is because the enlarged vocabulary is still size limited  and there s no solution for complementary when encountering unknown words  whereas the following category of methods can partly handle it  In addition  simply increasing the vocabulary size can merely bring little improve  ment due to the Zipfs Law  which means there is always a large tail of OOV words need to be addressed  48    2  methods by using context information  Besides the above variants which focus on computation speed up  a more ad  vanced category is using context information  Luong et al  proposed a word alignment algorithm which collaborates with  Copy Mechanism to post processing the translation result  This old but useful operation was inspired by the common word phrase  replacement method in SMT and has achieved a pretty considerable improvement in BLEU  59   Concretely  in Luong s method  for each of the OOV words  there s a  pointer  which map to the corresponding word in the source sentence  In the post processing stage  a predeﬁned dictionary was provided with  pointer  to ﬁnd the corresponding transla  tion  while using directly copy mechanism to handle the OOV words that not in the dictionary   The popularity of Luong et al   s method is partly because the Copy Mechanism actually provides an inﬁnite vocabulary  Further research has reﬁned this alignment algorithm for better replacement accuracy and generalization  Choi et al  extended Luong et al  s approach by dividing OOV words into one of three subdivisions based on their linguistic features   70  This method can help to remap the OOV words effectively  Gulcehre et al  done several reﬁnements in this category  they applied Copy Mechanism similar to Luong et al  but cooperate the Attention Mechanism in determining the location of word alignment  which is more ﬂexible in addressing alignment and could be directly utilized in other tasks which alignment loca  tion varies dramatically in both sides  like text summarization   Besides that  they synthesized Copy Mechanism with general translation operation by adding a so called switching network to decide which operation should be applied in each time  step  this could be thought to improve the generalization of the whole model   48   Gu et al  made parallel efforts in integrating different mechanisms  they proposed a kind of Attention Mechanism called CopyNet with the vanilla encoder decoder model  which can be naturally extended to handle OOV words in NMT task  74   Additionally  they found that the attention mechanism has driven more by the semantics and language model when using traditional word translation  but by location when using copying operation   Fig  11  An example of one kind of Copy Mechanism proposed by Luong et al   23   the subscripts of   unk   symbol  refer as d is a relative position of corresponding source words  where the alignment relation is a target word at position j is aligned to a source word at position i   j   d   Besides the Copy Mechanism  using extra knowledge is also useful in handling some other linguistic scenarios  which is highly related to the vocabulary coverage problem  Arthur et al  incorporated lexicons knowledge to assist with trans  lation in low frequency words  72   Feng et al  proposed a similar method with a memory augmented NMT  M NMT  architecture  and it used novel attention mechanism to get the extra knowledge from the dictionary that constructed by   SMT  73   Additionally  using context information can also be applied to improve the translation quality of ambiguous words  Homographs   75    In a nutshell  there are many context based reﬁnements have been proposed  most of them using Copy Mechanism to handle the OOV words with various of alignment algorithm to locate the corresponding word in the target side  However  these kinds of methods have a limited room for further improve  ment because the Copy Mechanism is too crude to handle the sophisticated scenarios in different languages  Practically  these methods perform poorly in languages which are rich morphology like Finnish and Turkish  which motivated method with better generalization  64    3  Methods in ﬁne grit level  This sub section introduce some more  intelligent  ways that focus on using additional information inside the word unit  It s obviously that such additional information could enhance the ability in covering the various of the linguistic phenomenon   In previous research  although using semantic information of word unit could provide the vast majority of learning features  other features in sub word level are generally ignored  From the perspective of linguistic  the concept of  word  is the basic unit of language but not the minimum one in containing semantic information  and there are abundant experienced rules could be learned from the inside of word units like shape and sufﬁx  Comparing with identity copy or dictionary translation which regards the rare words as an identical entity  the reﬁned method using ﬁne grit information is more adapt  able  Further  a more  radical  method was proposed  which just treats words in character level completely  It would be an innovative concern for future work   In this category  one popular choice is using the sub  word unit  the most remarkable achievement was proposed by Sennrich et al   which has been proved to have the best performance in some shared result  50   Concretely  in this design  they treat unknown words as sequences of sub word units  which is reasonable in terms of the composition of the vast majority of these words e g   named entities  loanwords  and morphologically complex words   In order to represent these OOV words completely  one intuitive solution is to build a predeﬁned sub word dictionary that contains enough variants of units  However  restoring the sub words cause massive space consumption in vocabulary size  which is effectively cancels out the whole purpose of reducing the computational efﬁciency in both time and space  Under this circumstances  a Byte Pair Encoding  BPE  based sub words extraction method was applied for word segmentation operation in both sides of languages  which successfully adapted this old but effective data compression method in text pre processing   In concrete details of this adapted BPE method  it alternated merging frequent pairs of bytes with characters or sequence of characters  and word segmentation process followed the below steps    1  Prepare a large training corpus generally are bilingual  corpus     2  Determined the size of the sub word vocabulary    3  Split the words to a sequence of characters  using a  special space character for marking the original spaces     4  Merge the most frequent adjacent pair of characters  e g    in English  this may be c and h into ch     5  Repeat step 4 until reaching the ﬁxed given number of times or the deﬁned size of the vocabulary  Each of these steps would increase the vocabulary by one   The ﬁgure shows a toy example of the method  As for the practical result of word segmentation  the most frequent words will be merged as single tokens  while the rare words which is similar to the OOV words in previous categories  work  may still contain un merged sub words  However  they have been found rare in processed text  50    Another notable  Further work of BPE method has also been proposed to obtaining a better generalization  Taku proposed subword regularization in later as an alternative to handle the spurious ambiguity phenomenon in BPE  and they further proposed a new subword segmentation algorithm based on a uni gram language model  which shares the same idea with BPE but was more ﬂexible in getting multiple segmentation based on their probabilities  Similarly  Wu et al  used  workpieces  concept to handle the OOV words which were once applied on the Google speech recognition system to solve Japanese Korean segmentation problem  60   24   This method breaks words into word pieces to get a balance between ﬂexibility with efﬁ  ciency when using single characters and full words separately  in characters level  Inspired by using character level information completely in building NLM  65   Costa jussa    Fonollosa used CNN with highway network to model the characters directly  62   they deployed this architecture in source side with a common word level generation in target side  Simi  larly  Ling et al  and Ballesteros et al  have proposed model respectively that using RNN LSTM  to build character level embedding and composes it into the word embedding  64   98   this idea later has been applied in building an RNN based character level NMT model  63   More recently  Luong and Manning 2016  proposed a hybrid model that combines the word level RNN with character level RNN for assist  99   Concretely  Luongs  method translates mostly at the word level  when encounter an OOV word  character level RNN would be used for the consult  The ﬁgure shows the detailed architecture of this model   concern is modeling sequence  On the other hand  the trying of designing a fully character  level translation model has also got attention accordingly  Chung et al  used BPE method to extract a sequence of subword in encoder side  they just varied the decoder by using pure characters  and it has indicated to provide comparable performance with models uses sub words  61   Motivated by aforementioned work  Lee et al  proposed fully character level NMT without any segmentation  it was based on CNN pooling with highway layers  which can solve the prohibitively slow speed of training in Luong and Manning s work  71     VI  ADVANCED MODELS  This section gives a demonstration of some advanced mod  els that have got the state of the art performance  while all of them belong to different categories of model structure  Experimental result these networks can achieve similar performance with different advantages in their corresponding aspects  A  ConvS2S  indicated that all  ConvS2S is short for Convolutional Sequence to Sequence  which is an end to end NMT model proposed by Gehring et al   82   Different with most of RNN based NMT models  ConvS2S is entirely CNN based model both in encoder and decoder  In the network structure  ConvS2S stacked 15 layers of CNN in its encoder and decoder with ﬁxed kernel width of 3  This deep structure helps to mitigate the weakness in capturing context information   In respect of network details  ConvS2S applied Gated Linear Units  GLU   100  in building network  which provide a gated function for output of convolution layer  Speciﬁcally  the output of convolution layer Y   R2d which is a vector with double times dimensions  2d numbers of dimensions  of each input element s embedding  d numbers of dimensions   the gated function processes the output Y    AB    R2d by implementing the equation 8  where both A and B are d dimensions vector  and the function σ B  is a gated function used to control which inputs A of the current context are relevant  This non linearity operation has been proved to be more effective in applying training language model  100    surpassing those only applying tanh function on A  140   In addition  ConvS2S also used residual connection  141  between different convolution layers   v  AB     A   σ B    9  Besides the innovation of CNN based encoder decoder struc  ture  ConvS2S also applied similar Attention Mechanism that has been wildly accepted by RNN model  called Multi  step Attention  Concretely  Multi step Attention is a separate attention structure applied in each decoder layer  In the process of calculating attention  the current hidden state dl i  i e   the output of the lth layer  has combined with previous output embedding gi as a vector of decoder state summary dl i   d   gi  i   bl   10  dhl dl i   W l Then the attention vector al ij  i e   the attention of state i with source element j in decoder layer l  would be driven by the dot product of the summary vector with the output of the ﬁnal encoder layer zu j    Lastly  the context vector is calculated as the weighted average of the attention vector al j as well as the encoder input ej   ij with the encoder output zu  al ij     cid 1  i   zu  t   cid 1   i   zu  j  exp cid 0 dl t 1 exp cid 0 dl  cid 80 m m cid 88   cid 0 zu  al ij  j   ej   cid 1   cl i    j 1  Fig  12  The structure of ConvS2S model  a successful CNN based NMT model with competitive performance to the state of the art  B  RNMT   RNMT  was proposed by Chen et al   103   This model has directly inherited the structure of GNMT model that was proposed by Wu et al   24   Speciﬁcally  RNMT  can be seen as an enhanced GNMT model  which demonstrated the best performance of RNN based NMT model  In model structure  RNMT  mainly differs from the GNMT model in the following several perspectives   First  RNMT  used six bi directional RNN  LSTM  in its decoder  whereas GNMT used one layer of bi directional RNN with seven layers of unidirectional RNN  This structure has sacriﬁced the computation efﬁciency in return for the extreme performance   Second  RNMT  applied Multi head additive attention in  stead of the single head attention in conventional NMT model  which can be seen as taking advantage of Transformer model  Third  synchronous training strategy was provided in the training process  which improved the convergence speed with model performance based on empirical results  102    In addition  inspired by Transformer model  per gate layer normalization  101  was applied  which has indicated to be helpful in stabilizing model training    11    12    where pos indicates the position and i indicates the dimension  That is  each dimension of the positional encoding corresponds to a sinusoid function  and the wavelengths form a geometric progression from 2π to 20000π  The reason of choosing these functions is that they have been assumed theoretically in helping the model to learn to attend by relative positions easily  due to the characters that for any ﬁxed offset k  P Epos   k can be represented as a linear function of P Epos    2  Multi head Self Attention Self Attention is the major innovation in Transformer  But in implementation  rather than just computing the Self Attention once  the multi head mechanism runs through the scaled dot  product attention multiple times in parallel  and the outputs of these independent attention are then concatenated and lin  early transformed into the expected dimensions  This multiple times Self Attention computation is called Multi head Self  Attention  which is applied for allowing the model to jointly attend to information from different representation sub spaces at different positions    3  Encoder   Decoder Blocks The encoder is built by 6 identical components  each of which contains one multi head attention layer with a fully connected network above it  These two sub layers are equipped with residual connection as well as layer normalization  All the sub layers have the same dimension of 512 in output data  The decoder  however  is more complicated  There are also 6 components stacked  and in each component  three sub  layers are connected  including two sub layers of multi head self attention and one sub layer of fully connected neural network  Speciﬁcally  the bottom attention layer is modiﬁed with method called masked to prevent positions from attending to subsequent positions  which is used for avoiding the model to look into the future of the target sequence when predicting the current word  Additionally  the second attention layer  the top attention layer  performs multi head attention over the output of the encoder stack   2  Transformer based NMT variants  Due to the tremen  dous performance improvement by Transformer  related reﬁne  ment has got huge attention for researchers  The well accepted weakness of vanilla Transformer includes  lacking of recur  rence modeling  theoretically not Turing complete  capturing position information  as well as large model complexity  All these drawbacks have hindered its further improvement of translation performance  In response to these problems  some adjustments have been proposed for getting a better model   In respect of model architecture  some proposed modi  ﬁcations focused on both in depth of attention layer and network composition  Bapna et al  has proposed 2 3x deeper Transformer with a reﬁned attention mechanism  which can be easier for the optimization of deeper models  152   The reﬁned attention mechanism extended its connection to each encoder layers  like a weighted residual connections along the encoder depth  which allows the model to ﬂexibly adjust the gradient ﬂow to different layers of encoder  Similarly  Wang et al   145  proposed a more deeper Transformer model  25 layers of encoder   which continues the same line of Bapna et  Fig  13  The structure of RNMT  model which has the similar structure of GNMT with adaptive innovation in Attention Mechanism  C  Transformer and Transformer based models  Transformer is a new NMT structure proposed by Vaswani et al   25   Different from existing NMT models  it has aban  doned the standard RNN CNN structures and designed an in  novative multi layer self attention blocks that are incorporated with a positional encoding method  This new trend of structure design takes the advantages from both RNN and CNN based model  which has been further used for initializing the input representation for other NLP tasks  Notably  Transformer is a complete Attention based NMT model   1  Structure of  the model  Transformer has its distinct structure in its model  where the major differences are the input representation and multi head attention    1  Input representation Transformer has its unique representation in handling input data that quite different with recurrent or convolution model  For computing Self Attention we mentioned  transformer han  dles the input as three kind of vectors for different purpose  They are Key Value and Query vectors  And all these vectors are driven by multiplying the input embedding with three matrices that we trained during the training process   Also  there s Positional Encoding method was applied for enhancing the modeling ability of sequence order  since Transformer has abandoned recurrence structure  this kind of method made a compensation by inject word order information in to feature vector  which can avoid the model to become invariant to sequence ordering  148   Speciﬁcally  Transformer add Positional Encoding to the input embedding at the bottoms of the encoder and decoder stacks  the Positional Encoding has been designed to have the same dimension with model embedding and thus could be summed  Positional Encoding can be calculated by applying positional functions directly or be learned  82   with be proven to have similar performance in ﬁnal evaluation  In Transformer  adding Positional Encoding by using since and cosine functions is ﬁnally chosen  and each position can be encoded in the following way   P E pos 2i    sin cid 0 pos 10000 2i dmodel   P E pos  2i 1    cos cid 0 pos 100002i dmodel cid 1    13    al  2018  s work with properly applying layer normalization and a novel output combination method   In contrast to the ﬁxed layers of NMT model  Dehghani et al  proposed Universal Transformers  which cancelled to stack the constant number of layers by combining recurrent inductive bias of RNNs and Adaptive Computation Time halting mechanism  thus enhanced the original self attention based representation for better learning iterative or recursive transformations  Notably  this adjustment has made the model be shown to be Turing complete under certain assumptions  142    As for reﬁnement in network composition  inspired by the thinking of AutoML  So et al  applied neural architecture search  NAS  to ﬁnd a comparable model with simpliﬁed architecture  146   The Evolved Transformer proposed in  146  has an innovative combination of basic blocks achieves the same quality as the original Transformer Big model with 37 6  less parameters   While most modiﬁcation is focus on changing model struc  ture directly  some new literature has chosen to utilize different input representation to improve model performance  One direct method is using enhanced Position Encoding for sequence order injection  where vanilla Transformer has weakness in capturing position information  Shaw et al  proposed modiﬁed self attention mechanism with awareness of utilizing repre  sentations of relative positions  which demonstrated to have a signiﬁcant improvements in two MT tasks  147    Concurrently  using pre initialized input representation with ﬁne tune is another orientation  where some attempt have been proposed in different NLP tasks such as applying ELMo  150  for encoder of NMT model  155   In terms of Transformer  one by product of this innovative model is using self attention for representing sequence  which can effectively fused word information with contextual information  In later  Two well  known Transformer based input representation methods were been proposed named Bert  Bidirectional Encoder Represen  tation from Transformers   149  and GPT  Generative Pre  trained Transformer   151   which has been indicated to bring improvement in some downstream NLP tasks  As for applying Transformer based pre trained model in NMT task  more recently  this kind of trying has also been realized by using Bert as additional embedding layer or applying Bert as pre  trained model directly  154   which has been indicated to provide a bit better performance than vanilla Transformer after ﬁne tune  Additionally  directly applying Bert as pre trained model has been proved to have similar performance and thus can be more convenient for encoder initialization   The full structure of Transformer is illustrated in Fig  14   VII  FUTURE TREND  Although we have witnessed the fast growing research pro  gresses in NMT  there are still many challenges  Based on the extensive investigation  121   125   117   116   we summarize the major challenges and list some potential directions in the following several aspects   Fig  14  The full structure of Transformer   1  In terms of translation performance  NMT still doesn t perform well in translating long sentences  This is mainly because of two reasons  the practical limitation in engineering and the learning ability of the model itself  For the ﬁrst reason  some academic experiments have chosen to ignore part of the long sentence that exceeds the RNN length  But we do believe that it s not the same thing when NMT has been deployed in industrial applications  For the second reason  as research progresses  the model architecture would be more compli  cated  For example  Transformer model has applied innovative structure in its design which brought signiﬁcant improvement in translation quality and speed  25   We believe that more reﬁnements in model structure would be proposed  As we all know that RNN based NMT takes its advantages in modeling sequence order but results in computational inefﬁciency  More future work would consider the trade off between these two aspects    2  Alignment mechanism is essential for both SMT and NMT models  For the vast majority of NMT models  Atten  tion Mechanism plays the functional role in the alignment task  while it arguably does broader work than conventional alignment models  We believe this advanced alignment method would still get attraction in future research  since powerful attention method can improve the model performance directly  Later research in attention mechanism would try to relieve the weakness in NMT such as interpretation ability  116     3  Vocabulary coverage problem has always affected most of the NMT models  The research trend in handling com  putation load of softmax operation would pursue  And we have also found new training strategy which supports large vocabulary size  Besides  research of NMT operating in sub  word or character level has also aroused in recent years  which provided additional solution beyond traditional scope    More importantly  solving sophisticated translation scenario such as informal spelling is also a hot spot  Current NMT model integrated with character level network has alleviated this phenomenon  Future work should focus on handling all kinds of OOV words in a more ﬂexible way    4  Low Resource Neural Machine Translation  125  is another hot spot in current NMT  which tries to solve the severe performance reduction when NMT model is trained with rare bilingual corpus  Since the aforementioned scenario happened commonly in practice where some seldom used languages don t have enough data  we do believe this ﬁled would be extended in further research  Multilingual transla  tion method  111   110  is the commonly proposed method which incorporated multi language pair of data to improve NMT performance  It may need more interpretation about the different results in choosing different language pairs  Besides  unsupervised method has utilized the additional dataset and provided pre trained model  Further research could improve its effect and provide hybrid training strategy with traditional method  122   123   124     5  Finally   research in NMT applications would also become more abundant  Currently  many applications have been developed such as speech translation  107   106  and document translation  105   We believe that various applications  especially end to end tasks  would emerge in the future  We strongly hope that an AI based simultaneous translation system could be applied in large scale  which can bring huge beneﬁt to our human society  108    level  REFERENCES   1  Klein  G   Kim  Y   Deng  Y   Senellart  J     Rush  A  M   2017   Opennmt  Open source toolkit for neural machine translation arXiv preprint arXiv 1701 02810    2  Forcada  M  L   Ginest Rosell  M   Nordfalk  J   ORegan  J   Ortiz Rojas  S   Prez Ortiz  J  A         Tyers  F  M   2011   Apertium  a free open  source platform for rule based machine translation  Machine translation  25 2   127 144    3  Koehn  P   Och  F  J     Marcu  D   2003  May   Statistical phrase  based translation  In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology Volume 1  pp  48 54   Association for Computational Linguistics    4  Koehn  P   Hoang  H   Birch  A   Callison Burch  C   Federico  M   Bertoldi  N         Dyer  C   2007  June   Moses  Open source toolkit for statistical machine translation  In Proceedings of the 45th annual meeting of the association for computational linguistics companion volume proceedings of the demo and poster sessions  pp  177 180     5  Chorowski  J   Bahdanau  D   Cho  K     Bengio  Y   2014   End to end continuous speech recognition using attention based recurrent nn  First results  arXiv preprint arXiv 1412 1602    6  Bengio  Y   Ducharme  R   Vincent  P     Jauvin  C   2003   A neural probabilistic language model  Journal of machine learning research  3 Feb   1137 1155    7  Cho  K   Van Merrinboer  B   Bahdanau  D     Bengio  Y   2014   On the properties of neural machine translation  Encoder decoder ap  proaches arXiv preprint arXiv 1409 1259    8  Kalchbrenner  N     Blunsom  P   2013   Recurrent continuous trans  lation models  InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pp  1700 1709     9  Mikolov  T   Chen  K   Corrado  G     Dean  J   2013   Efﬁcient estimation of word representations in vector space  arXiv preprint arXiv 1301 3781    10  Mikolov  T   Yih  W  T     Zweig  G   2013   Linguistic regularities in continuous space word representations  In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies  pp  746  751     11  Pennington  J   Socher  R     Manning  C   2014   Glove  Global vectors for word representation  In Proceedings of the 2014 conference on empirical methods in natural language processing  EMNLP   pp  1532  1543     12  Devlin  J   Zbib  R   Huang  Z   Lamar  T   Schwartz  R     Makhoul  J   2014   Fast and robust neural network joint models for statistical machine translation  In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics  Volume 1  Long Papers   Vol  1  pp  1370 1380     13  Galley  M   and Manning  C  D   2008  October   A simple and effective hierarchical phrase reordering model  In Proceedings of the Conference on Empirical Methods in Natural Language Processing  pp  848 856   Association for Computational Linguistics    14  Chiang  D   Knight  K     Wang  W   2009  May   11 001 new features for statistical machine translation  In Proceedings of human language technologies  The 2009 annual conference of the north american chapter of the association for computational linguistics  pp  218 226   Associa  tion for Computational Linguistics    15  Green  S   Wang  S   Cer  D     Manning  C  D   2013   Fast and adaptive online training of feature rich translation models  In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics  Volume 1  Long Papers   Vol  1  pp  311 321     16  Sutskever  I   Vinyals  O     Le  Q  V   2014   Sequence to sequence learning with neural networks  InAdvances in neural information pro  cessing systems pp  3104 3112     17  Hochreiter  S     Schmidhuber  J   1997   Long short term memory   Neural computation  9 8   1735 1780    18  Cho  K   Van Merrinboer  B   Gulcehre  C   Bahdanau  D   Bougares  F   Schwenk  H     Bengio  Y   2014   Learning phrase representations using RNN encoder decoder for statistical machine translation arXiv preprint arXiv 1406 1078    19  Gehring  J   Auli  M   Grangier  D     Dauphin  Y  N   2016   A con  volutional encoder model for neural machine translation arXiv preprint arXiv 1611 02344    20  Meng  F   Lu  Z   Wang  M   Li  H   Jiang  W     Liu  Q   2015   En  coding source language with convolutional neural network for machine translation arXiv preprint arXiv 1503 01838    21  Bahdanau  D   Cho  K     Bengio  Y   2014   Neural machine translation by jointly learning to align and translate arXiv preprint arXiv 1409 0473   22  Britz  D   Goldie  A   Luong  M  T     Le  Q   2017   Massive exploration of neural machine translation architectures arXiv preprint arXiv 1703 03906    23  Luong  M  T   Pham  H     Manning  C  D   2015   Effective ap  proaches to attention based neural machine translation arXiv preprint arXiv 1508 04025    24  Wu  Y   Schuster  M   Chen  Z   Le  Q  V   Norouzi  M   Macherey  W         Klingner  J   2016   Google s neural machine translation system  Bridging the gap between human and machine translation arXiv preprint arXiv 1609 08144    25  Vaswani  A   Shazeer  N   Parmar  N   Uszkoreit  J   Jones  L   Gomez  A  N         Polosukhin  I   2017   Attention is all you need  InAdvances in Neural Information Processing Systems pp  5998 6008     26  Chung  J   Gulcehre  C   Cho  K     Bengio  Y   2014   Empirical evaluation of gated recurrent neural networks on sequence modeling  arXiv preprint arXiv 1412 3555    27  Wu  Y   Zhang  S   Zhang  Y   Bengio  Y     Salakhutdinov  R  R   2016   On multiplicative integration with recurrent neural networks  In Advances in neural information processing systems  pp  2856 2864    28  Bengio  Y   Simard  P     Frasconi  P   1994   Learning long term dependencies with gradient descent is difﬁcult  IEEE transactions on neural networks  5 2   157 166    29  Graves  A   2012   Sequence transduction with recurrent neural net   works  arXiv preprint arXiv 1211 3711    30  Boulanger Lewandowski  N   Bengio  Y     Vincent  P   2013  Novem  ber   Audio Chord Recognition with Recurrent Neural Networks  In ISMIR  pp  335 340     31  Graves  A   2013   Generating sequences with recurrent neural networks   arXiv preprint arXiv 1308 0850     32  Koehn  P   2004  September   Pharaoh  a beam search decoder for phrase based statistical machine translation models  In Conference of the Association for Machine Translation in the Americas  pp  115 124   Springer  Berlin  Heidelberg    33  Chiang  D   2005  June   A hierarchical phrase based model for statisti  cal machine translation  In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics  pp  263 270   Association for Computational Linguistics    34  Och  F  J   Tillmann  C     Ney  H   1999   Improved alignment models for statistical machine translation  In 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora    35  Tu  Z   Lu  Z   Liu  Y   Liu  X     Li  H   2016   Modeling coverage for  neural machine translation  arXiv preprint arXiv 1601 04811    36  Auli  M   Galley  M   Quirk  C    Zweig  G   2013   Joint language and  translation modeling with recurrent neural networks    37  Auli  M     Gao  J   2014   Decoder integration and expected bleu  training for recurrent neural network language models    38  Schwenk  H     Gauvain  J  L   2005  October   Training neural network language models on very large corpora  In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing  pp  201 208   Association for Computational Linguistics    39  Schwenk  H   2007   Continuous space language models  Computer  Speech   Language  21 3   492 518    40  Schwenk  H   Dchelotte  D     Gauvain  J  L   2006  July   Continuous space language models for statistical machine translation  In Proceedings of the COLING ACL on Main conference poster sessions  pp  723 730   Association for Computational Linguistics    41  Mikolov  T   Karaﬁt  M   Burget  L   ernock  J     Khudanpur  S   2010   Recurrent neural network based language model  In Eleventh annual conference of the international speech communication association    42  Pollack  J  B   1990   Recursive distributed representations  Artiﬁcial  Intelligence  46 1 2   77 105    43  Chrisman  L   1991   Learning recursive distributed representations for  holistic computation  Connection Science  3 4   345 366    44  Allen  R  B   1987  June   Several studies on natural language and back  propagation  In Proceedings of the IEEE First International Conference on Neural Networks  Vol  2  No  S 335  p  341   IEEE Piscataway  NJ   45  Elman  J  L   1990   Finding structure in time  Cognitive science  14 2    179 211    46  JORDAN  M   1986   Serial Order  a parallel distributed processing  approach  ICS Report 8604  UC San Diego    47  Jean  S   Cho  K   Memisevic  R     Bengio  Y   2014   On using very large target vocabulary for neural machine translation  arXiv preprint arXiv 1412 2007    48  Gulcehre  C   Ahn  S   Nallapati  R   Zhou  B    Bengio  Y   2016    Pointing the unknown words  arXiv preprint arXiv 1603 08148    49  Jiajun  Z     Chengqing  Z   2016   Towards zero unknown word in  neural machine translation    50  Sennrich  R   Haddow  B     Birch  A   2015   Neural machine transla  tion of rare words with subword units arXiv preprint arXiv 1508 07909   51  Gage  P   1994   A new algorithm for data compression  The C Users  Journal  12 2   23 38    52  Mnih  A     Hinton  G  E   2009   A scalable hierarchical distributed lan  guage model  InAdvances in neural information processing systems pp  1081 1088     53  Morin  F     Bengio  Y   2005  January   Hierarchical probabilistic  neural network language model  InAistats Vol  5  pp  246 252     54  Miller  G   1998  WordNet  An electronic lexical database  MIT press   55  Bengio  Y     Sencal  J  S   2003  January   Quick Training of Proba   bilistic Neural Nets by Importance Sampling  InAISTATS pp  1 9     56  Bengio  Y     Sencal  J  S   2008   Adaptive importance sampling to accelerate training of a neural probabilistic language model IEEE Transactions on Neural Networks 19 4   713 722    57  Mnih  A     Teh  Y  W   2012   A fast and simple algorithm for training  neural probabilistic language models arXiv preprint arXiv 1206 6426    58  Vaswani  A   Zhao  Y   Fossum  V     Chiang  D   2013   Decoding with large scale neural language models improves translation  InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing pp  1387 1392     59  Luong  M  T   Sutskever  I   Le  Q  V   Vinyals  O     Zaremba  W   2014   Addressing the rare word problem in neural machine transla  tion arXiv preprint arXiv 1410 8206    60  Schuster  M     Nakajima  K   2012  March   Japanese and korean voice search  In2012 IEEE International Conference on Acoustics  Speech and Signal Processing  ICASSP  pp  5149 5152   IEEE    61  Chung  J   Cho  K     Bengio  Y   2016   A character level decoder with  out explicit segmentation for neural machine translation arXiv preprint arXiv 1603 06147    62  Costa Jussa  M  R     Fonollosa  J  A   2016   Character based neural  machine translation arXiv preprint arXiv 1603 00810    63  Ling  W   Trancoso  I   Dyer  C     Black  A  W   2015   Character based  neural machine translation arXiv preprint arXiv 1511 04586    64  Ling  W   Lus  T   Marujo  L   Astudillo  R  F   Amir  S   Dyer  C         Trancoso  I   2015   Finding function in form  Compositional character models for open vocabulary word representation arXiv preprint arXiv 1508 02096    65  Kim  Y   Jernite  Y   Sontag  D     Rush  A  M   2016  March   Character aware neural language models  InThirtieth AAAI Conference on Artiﬁcial Intelligence    66  Kudo  T   2018   Subword regularization  Improving neural network translation models with multiple subword candidates arXiv preprint arXiv 1804 10959    67  Chung  J   Gulcehre  C   Cho  K     Bengio  Y   2015  June   Gated feed  back recurrent neural networks  InInternational Conference on Machine Learning pp  2067 2075     68  Denkowski  M     Neubig  G   2017   Stronger baselines for trustable results in neural machine translation arXiv preprint arXiv 1706 09733   69  Nakazawa  T   Higashiyama  S   Ding  C   Mino  H   Goto  I   Kazawa  H         Kurohashi  S   2017  November   Overview of the 4th Workshop on Asian Translation  InProceedings of the 4th Workshop on Asian Translation  WAT2017  pp  1 54     70  Choi  H   Cho  K     Bengio  Y   2017   Context dependent word representation for neural machine translation Computer Speech   Lan  guage 45  149 160    71  Lee  J   Cho  K     Hofmann  T   2017   Fully character level neural machine translation without explicit segmentation Transactions of the Association for Computational Linguistics 5  365 378    72  Arthur  P   Neubig  G     Nakamura  S   2016   Incorporating dis  crete translation lexicons into neural machine translation arXiv preprint arXiv 1606 02006    73  Feng  Y   Zhang  S   Zhang  A   Wang  D     Abel  A   2017   Memory  augmented neural machine translation arXiv preprint arXiv 1708 02005   74  Gu  J   Lu  Z   Li  H     Li  V  O   2016   Incorporating copying mecha  nism in sequence to sequence learning arXiv preprint arXiv 1603 06393   75  Liu  F   Lu  H     Neubig  G   2017   Handling homographs in neural  machine translation arXiv preprint arXiv 1708 06510    76  Zhao  Y   Zhang  J   He  Z   Zong  C     Wu  H   2018   Addressing Troublesome Words in Neural Machine Translation  InProceedings of the 2018 Conference on Empirical Methods in Natural Language Pro  cessing pp  391 400     77  Vaswani  A   Bengio  S   Brevdo  E   Chollet  F   Gomez  A  N   Gouws  S         Sepassi  R   2018   Tensor2tensor for neural machine translation  arXiv preprint arXiv 1803 07416    78  Wang  Q   Li  B   Xiao  T   Zhu  J   Li  C   Wong  D  F     Chao  L  S   2019   Learning Deep Transformer Models for Machine Translation  arXiv preprint arXiv 1906 01787    79  So  D  R   Liang  C     Le  Q  V   2019   The evolved transformer  arXiv  preprint arXiv 1901 11117    80  Dehghani  M   Gouws  S   Vinyals  O   Uszkoreit  J     Kaiser     2018    Universal transformers  arXiv preprint arXiv 1807 03819    81  Dai  Z   Yang  Z   Yang  Y   Cohen  W  W   Carbonell  J   Le  Q  V     Salakhutdinov  R   2019   Transformer xl  Attentive language models beyond a ﬁxed length context  arXiv preprint arXiv 1901 02860    82  Gehring  J   Auli  M   Grangier  D   Yarats  D     Dauphin  Y  N   2017  August   Convolutional sequence to sequence learning  In Proceedings of the 34th International Conference on Machine Learning Volume 70  pp  1243 1252   JMLR  org    83  Hu  B   Tu  Z   Lu  Z   Li  H     Chen  Q   2015  July   Context  dependent translation selection using convolutional neural network  In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing  Volume 2  Short Papers   pp  536 541    84  Kalchbrenner  N   Espeholt  L   Simonyan  K   Oord  A  V  D   Graves  A     Kavukcuoglu  K   2016   Neural machine translation in linear time  arXiv preprint arXiv 1610 10099     85  Kaiser  L   Gomez  A  N     Chollet  F   2017   Depthwise sep  arable convolutions for neural machine translation  arXiv preprint arXiv 1706 03059    86  Kaiser       Bengio  S   2016   Can active memory replace attention   In Advances in Neural Information Processing Systems  pp  3781 3789    2018   The importance of being recurrent for modeling hierarchical structure  arXiv preprint arXiv 1803 03585    87  Tran  K   Bisazza  A     Monz  C    88  Zhou  J   Cao  Y   Wang  X   Li  P     Xu  W   2016   Deep recurrent models with fast forward connections for neural machine translation  Transactions of the Association for Computational Linguistics  4  371  383    89  Zhang  B   Xiong  D   Su  J   Duan  H     Zhang  M   2016   Variational  neural machine translation  arXiv preprint arXiv 1605 07869    90  Shazeer  N   Mirhoseini  A   Maziarz  K   Davis  A   Le  Q   Hinton  G     Dean  J   2017   Outrageously large neural networks  The sparsely  gated mixture of experts layer  arXiv preprint arXiv 1701 06538    91  Zhang  B   Xiong  D   Su  J   Lin  Q     Zhang  H   2018   Simplify  ing Neural Machine Translation with Addition Subtraction Twin Gated Recurrent Networks  arXiv preprint arXiv 1810 12546    92  Wang  M   Lu  Z   Zhou  J     Liu  Q   2017   Deep neural machine translation with linear associative unit  arXiv preprint arXiv 1705 00861   93  Feng  S   Liu  S   Yang  N   Li  M   Zhou  M     Zhu  K  Q   2016  December   Improving attention modeling with implicit distortion and fertility for machine translation  In Proceedings of COLING 2016  the 26th International Conference on Computational Linguistics  Technical Papers  pp  3082 3092     94  Yang  Z   Hu  Z   Deng  Y   Dyer  C     Smola  A   2016   Neural machine translation with recurrent attention modeling  arXiv preprint arXiv 1607 05108    95  Cohn  T   Hoang  C  D  V   Vymolova  E   Yao  K   Dyer  C     Haffari  G   2016   Incorporating structural alignment biases into an attentional neural translation model  arXiv preprint arXiv 1601 01085    96  Cheng  Y   Shen  S   He  Z   He  W   Wu  H   Sun  M     Liu  Y   2015   Agreement based joint training for bidirectional attention based neural machine translation  arXiv preprint arXiv 1512 04650    97  Mi  H   Wang  Z     Ittycheriah  A   2016   Supervised attentions for  neural machine translation  arXiv preprint arXiv 1608 00112    98  Ballesteros  M   Dyer  C     Smith  N  A   2015   Improved transition  based parsing by modeling characters instead of words with LSTMs  arXiv preprint arXiv 1508 00657    99  Luong  M  T     Manning  C  D   2016   Achieving open vocabulary neural machine translation with hybrid word character models  arXiv preprint arXiv 1604 00788    100  Dauphin  Y  N   Fan  A   Auli  M     Grangier  D   2017  August   Language modeling with gated convolutional networks  In Proceedings of the 34th International Conference on Machine Learning Volume 70  pp  933 941   JMLR  org    101  Ba  J  L   Kiros  J  R     Hinton  G  E   2016   Layer normalization   arXiv preprint arXiv 1607 06450    102  Chen  J   Pan  X   Monga  R   Bengio  S     Jozefowicz  R   2016   Re  visiting distributed synchronous SGD  arXiv preprint arXiv 1604 00981   103  Chen  M  X   Firat  O   Bapna  A   Johnson  M   Macherey  W   Foster  G         Wu  Y   2018   The best of both worlds  Combining recent ad  vances in neural machine translation  arXiv preprint arXiv 1804 09849   104  Mi  H   Wang  Z    Ittycheriah  A   2016   Vocabulary manipulation  for neural machine translation  arXiv preprint arXiv 1605 03209    105  Wang  L   Tu  Z   Way  A     Liu  Q   2017   Exploiting cross sentence context for neural machine translation arXiv preprint arXiv 1704 04347   106  Weiss  R  J   Chorowski  J   Jaitly  N   Wu  Y     Chen  Z   2017   Sequence to sequence models can directly translate foreign speech arXiv preprint arXiv 1703 08581    107  Duong  L   Anastasopoulos  A   Chiang  D   Bird  S     Cohn  T   2016  June   An attentional model for speech translation without transcription  InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics  Human Language Technologies pp  949 959     108  Gu  J   Neubig  G   Cho  K     Li  V  O   2016   Learning to translate in real time with neural machine translation arXiv preprint arXiv 1610 00388    109  Xu  K   Ba  J   Kiros  R   Cho  K   Courville  A   Salakhudinov  R         Bengio  Y   2015  June   Show  attend and tell  Neural image caption generation with visual attention  In International conference on machine learning  pp  2048 2057     110  Dong  D   Wu  H   He  W   Yu  D     Wang  H   2015  July   Multi  task learning for multiple language translation  InProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing  Volume 1  Long Papers  pp  1723 1732     111  Firat  O   Cho  K     Bengio  Y   2016   Multi way  multilingual neural machine translation with a shared attention mechanism arXiv preprint arXiv 1601 01073    112  Bojar  Ondrej  et al  Findings of the 2015 Workshop on Statistical Machine Translation  Proceedings of the Tenth Workshop on Statistical Machine Translation  2015  pp  146    113  Cettolo  Mauro  et al  The IWSLT 2015 Evaluation Campaign  IWSLT 2015  International Workshop on Spoken Language Translation  2015   114  Junczys Dowmunt  M   Dwojak  T     Hoang  H   2016   Is neural ma  chine translation ready for deployment  A case study on 30 translation directions  arXiv preprint arXiv 1610 01108    115  Bentivogli  L   Bisazza  A   Cettolo  M     Federico  M   2016   Neural versus phrase based machine translation quality  a case study  arXiv preprint arXiv 1608 04631    116  Chaudhari  S   Polatkan  G   Ramanath  R     Mithal  V   2019   An  attentive survey of attention models arXiv preprint arXiv 1904 02874    117  Galassi  A   Lippi  M     Torroni  P   2019   Attention  please  a critical review of neural attention models in natural language processing arXiv preprint arXiv 1902 02181    118  Domhan  T   2018  July   How much attention do you need  a granular analysis of neural machine translation architectures  InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics  Volume 1  Long Papers  pp  1799 1808     119  Kaiser       Sutskever  I   2015   Neural gpus learn algorithms arXiv  preprint arXiv 1511 08228    120  Lipton  Z  C   Berkowitz  J     Elkan  C   2015   A critical re  view of recurrent neural networks for sequence learning arXiv preprint arXiv 1506 00019    121  Koehn  P     Knowles  R   2017   Six challenges for neural machine  translation arXiv preprint arXiv 1706 03872    122  He  D   Xia  Y   Qin  T   Wang  L   Yu  N   Liu  T  Y     Ma  W  Y   2016   Dual learning for machine translation  InAdvances in Neural Information Processing Systems pp  820 828     123  Ramachandran  P   Liu  P  J     Le  Q  V   2016   Unsupervised pretrain  ing for sequence to sequence learning arXiv preprint arXiv 1611 02683   124  Artetxe  M   Labaka  G   Agirre  E     Cho  K   2017   Unsupervised  neural machine translation arXiv preprint arXiv 1710 11041    125  Sennrich  R     Zhang  B   2019   Revisiting Low Resource Neural  Machine Translation  A Case Study arXiv preprint arXiv 1905 11901    126  Schwenk  H   2012  December   Continuous space translation models for phrase based statistical machine translation  InProceedings of COL  ING 2012  Posters pp  1071 1080     127  Rosenfeld  R   2000   Two decades of statistical language modeling  Where do we go from here  Proceedings of the IEEE 88 8   1270 1278   128  Stolcke  A   2002   SRILM an extensible language modeling toolkit   InSeventh international conference on spoken language processing    129  Teh  Y  W   2006  July   A hierarchical Bayesian language model based on Pitman Yor processes  InProceedings of the 21st International Con  ference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics pp  985 992   Association for Computational Linguistics    130  Federico  M   Bertoldi  N     Cettolo  M   2008   IRSTLM  an open source toolkit for handling large scale language models  InNinth Annual Conference of the International Speech Communication Association    131  Heaﬁeld  K   2011  July   KenLM  Faster and smaller language model queries  InProceedings of the sixth workshop on statistical machine translation pp  187 197   Association for Computational Linguistics    132  Son  L  H   Allauzen  A     Yvon  F   2012  June   Continuous space translation models with neural networks  InProceedings of the 2012 conference of the north american chapter of the association for computational linguistics  Human language technologies pp  39 48   Association for Computational Linguistics    133  Collobert  R   Weston  J   Bottou  L   Karlen  M   Kavukcuoglu  K     Kuksa  P   2011   Natural language processing  almost  from scratch Journal of machine learning research 12 Aug   2493 2537    134  Young  T   Hazarika  D   Poria  S     Cambria  E   2018   Recent trends in deep learning based natural language processing ieee Computational intelligenCe magazine 13 3   55 75     135  Wallach  H  M   2006  June   Topic modeling  beyond bag of words  InProceedings of the 23rd international conference on Machine learn  ing pp  977 984   ACM    136  Song  F     Croft  W  B   1999  November   A general  language model for information retrieval  InProceedings of the eighth international conference on Information and knowledge management pp  316 321   ACM    137  Krizhevsky  A   Sutskever  I     Hinton  G  E   2012   Imagenet classiﬁcation with deep convolutional neural networks  InAdvances in neural information processing systems pp  1097 1105     138  Maas  A  L   Hannun  A  Y     Ng  A  Y   2013  June   Rectiﬁer nonlinearities improve neural network acoustic models  InProc  icml Vol  30  No  1  p  3     139  Cheng  Y   Shen  S   He  Z   He  W   Wu  H   Sun  M     Liu  Y  Agreement Based Joint Training for Bidirectional Attention Based Neural Machine Translation    140  Oord  A  V  D   Kalchbrenner  N     Kavukcuoglu  K   2016   Pixel  recurrent neural networks  arXiv preprint arXiv 1601 06759    141  He  K   Zhang  X   Ren  S     Sun  J   2016   Deep residual learning for image recognition  In Proceedings of the IEEE conference on computer vision and pattern recognition  pp  770 778     142  Dehghani  M   Gouws  S   Vinyals  O   Uszkoreit  J     Kaiser     2018    Universal transformers  arXiv preprint arXiv 1807 03819    143  Xiao  F   Li  J   Zhao  H   Wang  R     Chen  K   2019   Lattice Based Transformer Encoder for Neural Machine Translation  arXiv preprint arXiv 1906 01282    144  Hao  J   Wang  X   Yang  B   Wang  L   Zhang  J     Tu  Z   2019   Modeling recurrence for transformer  arXiv preprint arXiv 1904 03092   145  Wang  Q   Li  B   Xiao  T   Zhu  J   Li  C   Wong  D  F     Chao  L  S   2019   Learning Deep Transformer Models for Machine Translation  arXiv preprint arXiv 1906 01787    146  So  D  R   Liang  C     Le  Q  V   2019   The evolved transformer   arXiv preprint arXiv 1901 11117    147  Shaw  P   Uszkoreit  J     Vaswani  A   2018   Self attention with  relative position representations  arXiv preprint arXiv 1803 02155    148  Parikh  A  P   Tckstrm  O   Das  D     Uszkoreit  J   2016   A decomposable attention model for natural language inference  arXiv preprint arXiv 1606 01933    149  Devlin  J   Chang  M  W   Lee  K     Toutanova  K   2018   Bert  Pre  training of deep bidirectional transformers for language understanding  arXiv preprint arXiv 1810 04805    150  Peters  M  E   Neumann  M   Iyyer  M   Gardner  M   Clark  C   Lee  K     Zettlemoyer  L   2018   Deep contextualized word representations  arXiv preprint arXiv 1802 05365    151  Radford  A   Narasimhan  K   Salimans  T     Sutskever   language  Improving   2018   training  assets researchcovers languageunsupervised language paper  pdf   https   s3 us west 2   understanding  URL  by  amazonaws   generative  I  pre  com openai  understanding   152  Bapna  A   Chen  M  X   Firat  O   Cao  Y     Wu  Y   2018   Training deeper neural machine translation models with transparent attention  arXiv preprint arXiv 1808 07561    153  Guo  Q   Qiu  X   Liu  P   Shao  Y   Xue  X     Zhang  Z   2019    Star transformer  arXiv preprint arXiv 1902 09113    154  Clinchant  S   Jung  K  W     Nikoulina  V   2019   On the use of BERT  for Neural Machine Translation  arXiv preprint arXiv 1909 12744    155  Edunov  S   Baevski  A     Auli  M    2019   Pre trained lan  guage model representations for language generation  arXiv preprint arXiv 1903 09722    156  Luong  M  T   2017  Neural Machine Translation  Unpublished doctoral  dissertation  Stanford University  Stanford  CA 94305    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x667.491 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Topics found via LDA:\n",
            "\n",
            "Topic #0:\n",
            "word vector vocabulary time work conference processing long bengio size\n",
            "\n",
            "Topic #1:\n",
            "machine method sentence training information structure pp different network target\n",
            "\n",
            "Topic #2:\n",
            "sequence layer task data current end sub step section international\n",
            "\n",
            "Topic #3:\n",
            "translation et source process used input similar result development got\n",
            "\n",
            "Topic #4:\n",
            "neural words transformer learning applied new architecture hidden wu additional\n",
            "\n",
            "Topic #5:\n",
            "arxiv based language rnn original methods recurrent association layers better\n",
            "\n",
            "Topic #6:\n",
            "proposed mechanism deep smt oov networks problem cid character probability\n",
            "\n",
            "Topic #7:\n",
            "nmt al preprint encoder using research nlm length decoding statistical\n",
            "\n",
            "Topic #8:\n",
            "model models decoder alignment linguistics state representation self mt steps\n",
            "\n",
            "Topic #9:\n",
            "attention performance context cnn computational modeling position later zhang human\n",
            "0 2 0 2    r p A 7              E P o i b   q        1 v 3 5 1 4 0     4 0 0 2   v i X r a  Estimating the impact of preventive quarantine  with reverse epidemiology  Jacopo Grilli1  Matteo Marsili1  and Guido Sanguinetti2  1  The Abdus Salam International Center for Theoretical Physics  Trieste  Italy  2International School for Advanced Studies  SISSA   Trieste  Italy  April 2020  Abstract  The impact of mitigation or control measures on an epidemics can be estimated by ﬁtting the parameters of a compartmental model to empir  ical data  and running the model forward with modiﬁed parameters that account for a speciﬁc measure  This approach has several drawbacks  stemming from biases or lack of availability of data and instability of pa  rameter estimates  Here we take the opposite approach   that we call reverse epidemiology  Given the data  we reconstruct backward in time an ensemble of networks of contacts  and we assess the impact of mea  sures on that speciﬁc realization of the contagion process  This approach is robust because it only depends on parameters that describe the evo  lution of the disease within one individual  e g  latency time  and not on parameters that describe the spread of the epidemics in a population  Using this method  we assess the impact of preventive quarantine on the ongoing outbreak of Covid 19 in Italy  This gives an estimate of how many infected could have been avoided had preventive quarantine been enforced at a given time   1  Introduction  The spread of a pandemic such as Covid 19 may not be contained eﬃciently by social distancing measures alone  7   At least in part  this is because social distancing may not be possible in the household or in other circumstances  e g  in a hospital   In addition  lock down of entire countries is clearly unsustainable in the long run  for several reasons  Social distancing reduces the number of contacts of individuals  unconditionally on their morbidity state  Preventive quarantine is an additional measure where quarantine is applied to all the con  tacts of an infected individual as soon as the latter is tested positive  This may not stop contacts of the infected individual from developing the infection  but  1   it closes the possibility that  if the contact develops the infection  he or she will go on to infect further people   This measure   also called Centralised Quarantine   has been enforced in China  7  by establishing centralised quarantine centres  also1 for close contacts of conﬁrmed cases  to prevent further spread of the epidemics in case they also turned Covid19 positive   Ref   7  took a model based approach to study the impact of these measures a posteriori in the Wuhan epidemics  During the epidemics  this approach is not feasible  because data is scarce and unreliable  and consequently estimates of model parameters are aﬀected by considerable uncertainty  We apply a model  free strategy for using data available up to now  or possibly further extrapolated with plausible scenarios  to understand how the dynamics of the epidemics would have changed if preventive quarantine had been enforced at a particular time in the past  The idea is that the dynamics of the epidemics can be reversed backward  thereby constructing a probabilistic network of contagion contacts in the population of conﬁrmed cases  For each such reconstruction  the epidemics can be run forward again  with preventive quarantine enforced  This provides a picture of how much preventive quarantine would have aﬀected the epidemics  had it been implemented at a given time   2 Reversed epidemics  For preventive quarantine to work  it is essential to be able to trace contacts  Let us assume that the fraction of infected people who know by whom they have been infected is p  For countries where contact tracing apps have been introduced  p can be taken as a measure of the fraction of individuals that have that app installed and active on their mobile phone  Alternatively  contact tracing can be done by interviewing positives  a recent online survey of Spanish respondents  suggests that p be as high as 80  on the basis of individual interviews  4   Ferretti et al   1  estimate that if the adoption of a contact tracing app covers more that 70  of a population  then an epidemic with the traits of SARS CoV 2 can be tamed   We assume that  if A knows that B infected him her2  then B must have been tested positive in a previous day  We re also going to assume that  in this case  B also knows that he was in contact with A  With contact tracing apps  these are reasonable assumptions  otherwise this may be reasonable if the relationship between A and B is a signiﬁcant one  Let It the set of new positives at time t  Let Pt   It be the subset of infected at time t that know by whom they were infected  For each individual i   Pt we can simulate her previous history of contagion  she got infective 1Contacts of positive cases and symptomatic cases were quarantined in diﬀerent facilities   7    2By this we mean that A knows the identity of B  This does not count cases as  for example  when A knows that s he has been infected by a client of the grocery shop he she visited  It is unclear whether the ﬁgure of 80  reported in  4  includes such cases or not  2    with or without symptoms  at time t   Ti and the contact that passed them the infection occurred at time t   Ti   Li  where Ti  Li   1  2        are times of infection and latency and can be drawn from a distribution3  There is ample converging evidence on the expected values of Ti and Li for Covid 19  see  5  for a review   and also their distribution has been estimated from data  2   We assume that all those tested positive at time t do not infect others at time t or at later times  i e  they are immediately quarantined  or hospitalised   This may not be true in reality  From a time series of positive new cases  we can reconstruct at each time τ   t a population Cτ of contacts  which is the set of people that got infected at time τ   and the population of Eτ of infective  but not yet detected  individuals  Eτ contains all those individuals for which ti   Ti   τ   ti  where ti is the time where i was tested positive  For each i   Cτ we can draw at random a link  i e  a contagion event  to a j   Eτ and reconstruct the contact network at time τ   This construction4 gives a possible history of how the contagion has spread up to time t  For all i   Eτ we deﬁne  i τ as the set of all j   Cτ that were infected by i at time τ   It is important to remark that this is only a plausible history of the contagion  nevertheless  if the statistics of the latency  detection times and traceability are reasonable  we expect that conclusions drawn from multiple simulated histories will provide a faithful representation of the average behaviour of the epidemic  Now let us assume that at time tpq a preventive quarantine measure is put in place  This implies that all individuals that got in contacts of all i   Iτ since τ   tQ are isolated  with tQ   14 days   We generate a set Qτ that contains all individuals in quarantine   cid 91    cid 91   τ cid 91    i τ cid 48   i Pτ  τ cid 48  τ tQ        Qτ   Qτ 1   1   In words  Qτ contains all individuals in quarantine the day before  plus all those that where in contact with all i   Pτ in the previous tQ  Notice there is no need to remove people from Qτ   since we re considering only positives that stay in quarantine until they become negative  As a consequence all contagion events involving i   Qτ are eliminated  After this is done  the network of contagion is reconstructed on all the remaining contagion events  a new population P Q  τ is reconstructed   3Uppercase letters are used for random variables  4This corresponds to sampling the maximum entropy ensemble of contact networks consis  tent with the data  This is appropriate in the absence of further information on the statistics of contacts  The approach can be reﬁned to include additional information on contact distri  butions   3   3 Results  The result of the reconstruction5 of the epidemics is shown in Figure 1  Data from Friuli Venezia Giulia  FVG  and Veneto regions are compared to what would have happened on average if preventive quarantine had been enforced on March 18th  day 24   We extend the data up to April 5th with a possible scenario of how the epidemics may evolve under a slowly varying downward trend 6  This sheds light on the eﬀects of preventive quarantine on a longer timescale  We assumed a geometric distribution for the distributions of Ti and Li  with E Ti    3 and E Li    5   Figure 1  Top  New positive Covid 19 cases  black lines  for the FVG  top left  and Veneto  top right  regions  The data up to April 4th  vertical line at day 41  is continued by simulating a possible scenario  This dataset is com  pared with the new positives under a preventive quarantine measure  p   0 8  enforced on March 18th  vertical line at day 24  in 100 independent runs of the reconstructed epidemics  green dots   Averages are reported as red lines  Bottom left  Average new positives in 100 runs of he reconstructed epidemics with preventive quarantine for diﬀerent values of p for the FVG Region  Bottom right  Fraction of the new positives that should be dealt with with preventive quarantine for diﬀerent values of p for the Veneto Region   After suﬃcient time  the ratio between the predicted new positives under  5The code used for the simulations is available under request at marsili ictp it  6Notice that these are not predictions  they are simply illustrative of a possible forward  history of the epidemic and the relative eﬀect that preventative quarantine would have   4   0 20 40 60 80 100 120 140 160 0 10 20 30 40 50 60 70 80new positivesdays since 24 2Data FVGPreventive Quarantine  p 0 8  0 100 200 300 400 500 600 0 10 20 30 40 50 60 70 80new positivesdays since 24 2Data VenetoPreventive quarantine  p 0 8  0 20 40 60 80 100 120 140 160 0 10 20 30 40 50 60 70 80new positivesdays since 24 2Data FVGp 0 5p 0 6p 0 7p 0 8p 0 9p 0 95 0 0 2 0 4 0 6 0 8 1 0 10 20 30 40 50 60 70 80  positivesdays since 24 2p 0 95p 0 9p 0 8p 0 7p 0 6p 0 5 preventive quarantine and the observed ones approach a constant  The value of this proportion can be easily calculated by considering that new cases under preventive quarantine are possible only if a person does not know by whom was infected  event that happens with probability p  or if she knows that was infected by someone who was not under preventive quarantine  which happens with probability p 1   p    For large times the ratio of new positive with and without preventive quarantine tends to 1   p2   4 Discussion  There are a number of aspects worth mentioning     Individuals i   Iτ  Pτ who don t know whom they ve been infected by are not removed  This is a clear incentive for adopting apps at individual level  We assume that individuals who don t know whom they are infected by  are infected by individuals that do not belong to the population of detected positives     We assume that if i knows that he she was infected by j then j knows  at the time of being tested positive  that he she had a contact with i  This may not be true based on questionnaire  but it is reasonable if apps for contact tracing are adopted     The set Qτ only includes positives in quarantine  It does not include those who got in contact with a positive case  but who were not infected  So the set of quarantined people should be much larger  Ten times the number of positives may be a a conservative measure     The reverse epidemics approach relies on parameters of the infection dy  namics  the distributions of Ti and Li  that have been estimated empir  ically  7  5   In particular  the approach is independent of the infection rate  percentage of asymptomatic cases and other parameters that make predictions on the basis of forward epidemiological models very unstable    Eﬀects of preventive quarantine are weak in the ﬁrst week  see Fig  1  due to the lags related to incubation period and detection  A signiﬁcant eﬀect is apparent by the end of the second week  The full eﬀect sets in after the third week     The eﬀects of preventive quarantine depend strongly on p  the eﬃciency with which contacts are monitored  An eﬃciency p   0 8 in tracing con  tacts  that according to a recent study should be achievable by interview  ing positive cases  4   can reduce the size of the epidemics by 60   An eﬃciency of 95   such as that reported by Wang at al   7  for Wuhan  can reduce almost to 10  of what they would have been  If mobile apps will be adopted the level of compliance of users will be critical to ensure that preventive quarantine is eﬀective   5     Our analysis permits us to estimate the eﬀects of preventive quarantine on the tested positive population in a counterfactual scenario  In reality  preventive quarantine would also isolate many infected individuals who would not go on to develop symptoms  and hence would be unlikely to be tested  but who would contribute to further spread the disease  The isolation of non tested individuals would contribute  in the most optimistic scenario  to a reduction of basic reproductive number of a factor 1  p2  if all the contacts between people that know who they were infected by are eliminated  This optimistic reduction of the basic reproductive number would correspond to an exponential decrease of the ratio between new cases with and without preventive quarantine  The protective potential of preventive quarantine might be therefore between this latter optimistic scenario and the more pessimistic one that we discussed in this paper and illustrated in Figure 4   The reverse epidemiology approach bears similarity with the reconstruction of phylogeny from generic data  see e g   3   and the counterfactual analysis of disruption events in other complex systems  see e g   6    We are not aware of any such approach to the study of epidemic outbreaks  which we believe is a very promising one   References   1  Ferretti  L   Wymant  C   Kendall  M   Zhao  L   Nurtay  A   Abeler D orner  L   Parker  M   Bonsall  D   and Fraser  C  Quantifying sars cov 2 transmission suggests epidemic control with digital contact tracing  Science  2020     2  Linton  N   Kobayashi  T   Yang  Y   Hayashi  K   Akhmetzhanov  A   Jung  S  M   Yuan  B   Kinoshita  R   and Nishiura  H  Incuba  tion Period and Other Epidemiological Characteristics of 2019 Novel Coro  navirus Infections with Right Truncation  A Statistical Analysis of Publicly Available Case Data  J  Clin  Med  9  2020   538    3  McCloskey  R  M   Liang  R  H   and Poon  A  F  Reconstructing contact network parameters from viral phylogenies  Virus Evolution 2  2  10 2016   vew029    4  Oliver  N   Barber  X   Roomp  K   and Roomp  K   The Covid19Impact Survey  Assessing the Pulse of the COVID 19 Pandemic in Spain via 24 questions  arXiv e prints  Apr  2020   arXiv 2004 01014    5  Pellis  L   Scarabel  F   Stage  H  B   Overton  C  E   Chap  pell  L  H   Lythgoe  K  A   Fearon  E   Bennett  E   Curran  Sebastian  J   Das  R   et al  Challenges in control of covid 19  short doubling time and long delay to eﬀect of interventions  arXiv preprint arXiv 2004 00117  2020    6    6  Silva  R   Kang  S  M   and Airoldi  E  M  Predicting traﬃc volumes and estimating the eﬀects of shocks in massive transportation systems  Pro  ceedings of the National Academy of Sciences 112  18  2015   5643 5648    7  Wang  C   Liu  L   Hao  X   Guo  H   Wang  Q   Huang  J   He  N   Yu  H   Lin  X   Pan  A   Wei  S   and Wu  T  Evolving epidemi  ology and impact of non pharmaceutical interventions on the outbreak of coronavirus disease 2019 in wuhan  china  medRxiv  2020    7   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x667.491 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Topics found via LDA:\n",
            "\n",
            "Topic #0:\n",
            "cases number history epidemiology italy wuhan does reduce signiﬁcant line\n",
            "\n",
            "Topic #1:\n",
            "preventive individuals forward wang contains average diﬀerent right contribute iτ\n",
            "\n",
            "Topic #2:\n",
            "contact data qτ possible people day got available control measures\n",
            "\n",
            "Topic #3:\n",
            "contagion tracing enforced covid times apps epidemic th positivesdays network\n",
            "\n",
            "Topic #4:\n",
            "time epidemics new assume li eﬀect tq values quarantined disease\n",
            "\n",
            "Topic #5:\n",
            "quarantine infected ti set infection impact spread eﬀects scenario reconstruct\n",
            "\n",
            "Topic #6:\n",
            "measure individual case week estimate figure study period adopted region\n",
            "\n",
            "Topic #7:\n",
            "contacts know let analysis ratio april social particular happens empir\n",
            "\n",
            "Topic #8:\n",
            "positives parameters population model cid reconstructed reasonable reverse eﬃciency fraction\n",
            "\n",
            "Topic #9:\n",
            "positive approach tested knows eτ arxiv latency estimated don sars\n",
            "Draft version April 10  2020 Preprint typeset using LATEX style emulateapj v  12 16 11  0 2 0 2    r p A 8               A G h p   o r t s a        1 v 7 6 1 4 0     4 0 0 2   v i X r a  RADIO POWER FROM A DIRECT COLLAPSE BLACK HOLE IN CR7  Daniel J  Whalen1 2  Mar Mezcua3  Avery Meiksin4  Tilman Hartwig5 6 7 and Muhammad A  Latif8  Draft version April 10  2020  ABSTRACT  The leading contenders for the seeds of the ﬁrst quasars are direct collapse black holes  DCBHs  formed during catastrophic baryon collapse in atomically cooled halos at z   20  The discovery of the Lyα emitter CR7 at z   6 6 was initially held to be the ﬁrst detection of a DCBH  although this interpretation has since been challenged on the grounds of Spitzer IRAC and Very Large Telescope X Shooter data  Here we estimate the radio ﬂux from a DCBH and a young supernova remnant in CR7  the latter of which can be confused with ﬂux from a quasar  We ﬁnd that a DCBH would emit a ﬂux of 0 75   8 9 µJy at 1 0 GHz  far greater than the nJy signal expected for a young supernova  so the detection of any radio emission from CR7 would conﬁrm it to be the potential site of a DCBH  This ﬂux could easily be detected by the next generation Very Large Array and the Square Kilometer Array in the coming decade  Subject headings  quasars  supermassive black holes   black hole physics   early universe   dark ages   reionization  ﬁrst stars   galaxies  formation   galaxies  high redshift  1  INTRODUCTION  Over 300 quasars have now been discovered at z   6  including seven at z   7  Mortlock et al  2011  Ba nados et al  2018  Matsuoka et al  2019   The seeds of these quasars may be supermassive primordial stars that die as direct collapse black holes  DCBHs  at z   20  They form when atomic cooling in a 107   108 M  metal  free halo triggers catastrophic baryon collapse at its cen  ter  with infall rates of up to 1 M  yr 1  e g   Wise et al  2008  Regan   Haehnelt 2009  Latif et al  2013   These inﬂows build up a single star that later collapses to a BH at a mass of   105 M   Hosokawa et al  2013  Woods et al  2017  Haemmerl e et al  2018a b   DCBHs are the leading candidates for the seeds of the ﬁrst quasars because they are born in high accretion rates in which they grow more quickly than normal Pop III star BHs  which form in much lower densities  e g   Whalen et al  2004   Less massive Pop III star BHs are also subject to natal kicks that can eject them from their halos  Whalen   Fryer 2012  and do not encounter enough gas at later times to fuel their rapid growth  Alvarez et al  2009  Smith et al  2018   see also Mezcua 2017  Woods et al  2019    The discovery of the strong Lyα emitter CR7 at z   6 6  Bowler et al  2012  Matthee et al  2015  was originally  1 Institute of Cosmology and Gravitation  Portsmouth Uni   versity  Dennis Sciama Building  Portsmouth PO1 3FX  2 Ida Pfeifer Professor  Department of Astrophysics  Univer  sity of Vienna  Tuerkenschanzstrasse 17  1180  Vienna  Austria 2 Institute of Space Sciences  ICE  CSIC   Campus UAB  Car   rer de Magrans  08193 Barcelona  Spain  3 Institut d Estudis Espacials de Catalunya  IEEC   Carrer  Gran Capit a  08034 Barcelona  Spain  4 Institute for Astronomy  University of Edinburgh  Blackford  Hill  Edinburgh EH9 3HJ  UK  5 Kavli IPMU  WPI   UTIAS  The University of Tokyo   Kashiwa  Chiba 277 8583  Japan  6 Department of Physics  School of Science  University of  Tokyo  Bunkyo  Tokyo 113 0033  Japan  7 Institute for Physics of Intelligence  School of Science  The  University of Tokyo  Bunkyo  Tokyo 113 0033  Japan  8 Physics Department  College of Science  United Arab Emi   rates University  PO Box 15551  Al Ain  UAE  held by some to be the ﬁrst detection of a DCBH  or a Pop III galaxy  Sobral et al  2015  Pallottini et al  2015  because of the detection of He II 1640  A emission and the absence of metal lines in the initial observations  Subse  quent analyses favored a DCBH because of the diﬃculties associated with forming 107 M  of Pop III stars at the lower limit of metallicity imposed by observations at the time  Hartwig et al  2016   But this interpretation has since been challenged on the grounds of  OIII  4959  A and 5007  A emission in Spitzer IRAC data  Bowler et al  2017    CII  158 µm emission found by the Atacama Large Millimeter Array  ALMA  Matthee et al  2017   and the re analysis of Very Large Telescope  VLT  X  Shooter data  Shibuya et al  2018   which failed to con  ﬁrm the presence of He II recombination line emission  In particular  the presence of oxygen and carbon was thought to rule out a DCBH in CR7 because they form in zero metallicity environments   However  population synthesis and spectral ﬁtting models predict masses of 5   10 million M  for a BH in CR7  well above those of DCBHs at birth  suggesting if one did form in CR7 it had since grown by up to a factor 100 in mass  Agarwal et al  2016  Pacucci et al  2017a   If so  one would expect the existence of metals in CR7 because X rays from the BH are known to trigger star formation and supernova  SN  explosions in its vicinity  Secondary ionizations from energetic photoelectrons en  hance free electron fractions and H2 formation in the gas  which then cools and forms stars  e g   Machacek et al  2003   Metals or dust could also obscure He II recombi  nation line emission from the BH   Radio observations could conﬁrm the likely presence of a DCBH in CR7 because it could emit synchrotron radi  ation that could be detected by the next generation Very Large Array  ngVLA  or epoch of reionization  EoR  ob  servatories such as the Low Frequency Array  LOFAR  or the Square Kilometer Array  SKA   Recent studies indicate that the ampliﬁcation of seed magnetic ﬁelds by turbulent dynamos could create tangled magnetic ﬁelds even in primordial accretion disks  Schober et al    2  TABLE 1  Fundamental Planes  FP  α  β  γ  MER03 KOR06 GUL09 PLOT12 BON13  0 60 0 71 0 67 0 69 0 39  0 78 0 62 0 78 0 61 0 68  7 33 3 55 4 80 4 19 16 61  2012   These ﬁelds can then be ordered and further am  pliﬁed by the rotation of the disk by the αΩ dynamo  Latif   Schleicher 2016  and emit strong radio ﬂuxes upon birth of the BH  However  a young SN remnant from a starburst could masquerade as a DCBH because SNe can also emit large synchrotron ﬂuxes at early times  We have now calculated the radio ﬂuxes for a DCBH and a young SN remnant in CR7  In Section 2 we de  scribe our empirical estimates of the ﬂux from a DCBH derived from several fundamental planes of BH accretion  We calculate the radio ﬂux due to synchrotron emission from a young SN remnant in Section 3  In Section 4 we compare these ﬂuxes to determine if a young SN could be confused with a BH in CR7 or if the detection of any radio emission from CR7 would conﬁrm the existence of a BH there   2  RADIO POWER FROM A DCBH  Observations have empirically conﬁrmed a correlation between the mass of a BH  MBH  its 2   10 keV nu  clear X ray luminosity  LX  and its 5 GHz nuclear lumi  nosity  LR  known as the fundamental plane of BH ac  cretion  Merloni et al  2003  see Mezcua et al  2018 for a brief review   This correlation is supported by theo  retical models of accretion and extends over six orders of magnitude in mass  including the intermediate mass black hole  IMBH  regime  G ultekin et al  2014   There has been some debate if radio loud and radio quiet ac  tive galactic nuclei  AGNs  occupy distinct regions in the FP but La Franca et al   2010  and Bonchi et al   2013  have found no evidence for a bimodality in the radio lu  minosity function and that the FP is applicable to all types of AGNs   To estimate the ﬂux from a DCBH in CR7 in a given radio band in the observer frame we ﬁrst calculate its 5 GHz luminosity in the rest frame with a FP  This requires LX  which we ﬁnd from the bolometric luminosity of the BH with Equation 21 of Marconi et al   2004    log cid 18  Lbol  LX  cid 19    1 54   0 24L   0 012L2   0 0015L3   1   where L   log Lbol   12  Agarwal et al   2016  estimate the mass and luminosity of the BH in CR7 to be 4 4   106 M  and 0 4 LEdd  respectively  where LEdd   1 26   1038  M M   erg s 1  These values yield LX   1 22   1043 erg s 1  which is consistent with the upper limit LX   1044 erg s 1 found by Pacucci et al   2017b   LR can then be obtained from any of a number of FPs of the form  log LR   α log LX   β log MBH   γ    2   where α  β and γ for FPs from Merloni et al   2003  MER03   K ording et al   2006  KOR06   G ultekin et al   2009  GUL09   Plotkin et al   2012  PLT12   and  TABLE 2  CR7 DCBH Radio Fluxes  µJy   FP  150 MHz  1 0 GHz  3 0 GHz  8 0 GHz  MER03 KOR06 GUL09 PLT12 BON13 GUL19  11 0 2 84 33 6 4 45 4 05 16 3  2 91 0 75 8 90 1 18 1 07 4 33  1 35 0 35 4 12 0 55 0 49 2 01  0 68 0 18 2 08 0 28 0 25 1 01  102  LoTSS  10 0    y J µ     x u  l f  1 0  SKA MID wide  VLASS  MER03 KOR06 GUL09 PLT12 BON13 GUL19  SKA MID narrow  ngVLA  0 1  0 1  1 0  frequency  GHz   10 0  Fig  1   CR7 DCBH ﬂuxes predicted by the six FPs from 100 MHz   10 GHz  Detection limits for LoTSS  150 MHz   SKA MID wide and narrow ﬁelds  1 GHz   VLASS  3 0 GHz  and the ngVLA  8 GHz  are marked by triangles   Bonchi et al   2013  BON13  are listed in Table 1  We also include the FP of G ultekin et al   2019  GUL19    R    0 62   0 70 X   0 74 µ    3   where R   log LR 1038erg s   X   log LX 1040erg s  and µ   log MBH 108M     Radio ﬂux from CR7 that is redshifted into a given band in the observer frame in general does not originate from 5 GHz in the source frame so we calculate it from LR   νLν  assuming that the spectral luminosity Lν   ν α with a spectral index α   0 7  Condon et al  2002   The spectral ﬂux at ν in the observer frame can then be obtained from the spectral luminosity at ν   in the rest frame from  Fν    Lν     4π 1   z dA  2     4   where dA is the angular diameter distance and ν      1   z ν  If we use second year Planck cosmologi  cal parameters  ΩM   0 308  ΩΛ   0 691  Ωbh2   0 0223  σ8   0 816  h   0 677 and n   0 968  Planck Collaboration et al  2016   dA   1143 8 Mpc   We list DCBH ﬂuxes for CR7 at ν   150 MHz  1 GHz  3 GHz and 8 GHz corresponding to the LOFAR Two  Metre Sky Survey  LoTSS   SKA MID  the Very Large Array Sky Survey  VLASS  and the ngVLA in Table 2 and plot them for all frequencies from 100 MHz   10 GHz in Figure 1  The current sensitivity of VLASS is 120   3  1  0 1  0 01  1  0 1  0 01  0 001  0 1  1  10  0 001  0 1  1  10  Fig  2   Radio synchrotron emission by a 15 M  CC SN at z   6 6  Left  ambient density of 0 1 cm 3  The curves are for frequencies of 0 5 GHz  dotted   1 4 GHz  solid   3 GHz  short dashed  and 8 4 GHz  long dashed   Right  ambient density of 0 4 cm 3  The curves are 3 GHz  solid   8 4 GHz  short dashed   10 GHz  long dashed  and 25 GHz  dot dashed   Here  pe is the index of the power law of the energy distribution of the relativistic electrons and fe is the fraction of the thermal energy of the shock that goes into relativistic electrons  Section 2 of Meiksin   Whalen 2013    µJy but will improve to 59 µJy after all three passes are completed by 2024  LoTSS will reach 71 µJy and SKA wide ﬁeld and narrow surveys will reach sensitivities of 1 µJy beam and 0 2 µJy beam  respectively  ngVLA will achieve sensitivities of 0 22 µJy beam for 1h integra  tion times  The DCBH ﬂuxes fall well below LoTSS and VLASS detection limits but could be found by deeper LOFAR and VLA surveys in the future  Most of the ﬂuxes could be detected by SKA wide ﬁeld and narrow surveys and by ngVLA   3  SN RADIO FLUX  The upper limit to radio power from SNe in CR7 would come from a starburst because it would produce the high  est explosion rate  In such an event most of the stars in the cluster form in about the lifetime of any one of them  and 10   20 M  core collapse  CC  SNe would produce the most synchrotron emission because energetic pair  instability  PI  SNe explode in much lower density H II regions that emit far less radio energy when swept up by the remnant  Meiksin   Whalen 2013   Elemental abun  dances measured in a number of extremely metal poor stars suggest that many stars in the early Universe may have been a few tens of solar masses  e g   Joggerst et al  2010  Ishigaki et al  2018   One upper limit to the SN rate for a starburst in CR7 can be obtained by dividing the total mass of Pop III stars originally inferred to be in CR7    107 M   by the average mass of the longest lived stars capable of producing SNe  which we take to be   15 M   This yields the maximum number of SNe over the duration of the burst  which would be about the life  time of a 15 M  Pop III star    10 Myr  Schaerer 2002   Dividing the total number of SNe by the duration of the burst yields one Pop III SN every 15 yr   We show the observed  Earth frame  radio ﬂux in  several bands for a 15 M  CC SN at z   6 6 in Fig  2  for a detailed description of this calculation  see Meiksin   Whalen 2013   In the left and right panels we show the radio ﬂuxes for explosions in two ﬁducial H II regions denoted halo 1 and 2  respectively  The ambient density of the explosion is 0 1 cm 3 in halo 1 and 0 4 cm 3 in halo 2  On the left we show the ﬂux at 0 5 GHz  1 4 GHz  3 GHz and 8 4 GHz and on the right we show the ﬂux at 3 GHz  8 4 GHz  10 GHz and 25 GHz  In both cases the ﬂux is nearly constant out to 0 3 yr and then falls by three orders of magnitude by 2 yr  Over these times the two explosions yield very similar radio ﬂuxes  which vary from 0 7   2 5 nJy over the ﬁrst 0 3 yr  Although both explosions peak above 1 nJy  their average ﬂux over the 15 yr interval between explosions in the starburst would be at least a factor of 10 lower because of its sharp decline after less than a year   Another upper limit to the SN rate can be derived from the Pop II star formation rate  SFR  inferred from recent UV and IR observations of CR7 by ALMA  which is   50 M  yr 1  Table 1 of Matthee et al  2017   Assuming a Salpeter IMF with one CC SN per 60 M  of stars  this SFR produces about one CC SN per year  a rate about 15 times that obtained from Pop III stars  Given that the average signal from each SN over its ﬁrst year is about 1 nJy before falling oﬀ sharply  a Pop II SFR would yield   1 nJy of continuous ﬂux  still about 1000 times lower than would be expected from a DCBH   4  DISCUSSION AND CONCLUSION  Our calculations indicate that the detection of any ra  dio emission from CR7 would conﬁrm the presence of a DCBH candidate there because the sub nJy ﬂux of a young SN remnant would fall far below the detection limit of any current or planned survey  Furthermore    4  we ﬁnd that ﬂux from a DCBH in CR7 could be eas  ily detected in a number of LOFAR  SKA and ngVLA bands in future surveys  The VLA has already visited the COSMOS legacy ﬁelds in which CR7 was originally discovered at 3 GHz with a sensitivity of 2 3 µJy beam  Smolˇci c et al  2017   but we found no radio counterpart to CR7 in this archive  This is not surprising  given that only one FP predicts a ﬂux greater than 2 3 µJy at this frequency   SN remnants with much higher radio ﬂuxes than those in our study have been discovered in the local universe  such as SNR 4449 1    6 mJy  Mezcua et al  2013   but such signals would be highly unlikely from the interior of CR7  SNe there occur in low metallicity environments in which there is less dust to block UV radiation from the progenitor star from completely ionizing the cloud core that gave birth to it  Once ionized  supersonic core ﬂows expand outward from its center and disperse it before the star dies  leaving it to explode in very low ambient den  sities from which there is not much synchrotron emission  Whalen et al  2004   Massive stars forming in dust and gas at nearly solar metallicities today die in much higher ambient densities from which there can be much more radio emission from the early remnant   A unique aspect of high redshift quasars is that the cosmic microwave background  CMB  can quench ra  dio emission from BH jets  If the energy density of CMB photons exceeds that of the magnetic ﬁelds in  the lobes of the jet  relativistic electrons preferentially cool by upscattering CMB photons rather than by syn  chrotron radiation  and the lack of radio emission from some high redshift quasars has been attributed to this process  Ghisellini et al  2014  Fabian et al  2014   How  ever  this would not change the ﬂuxes in our calculations because they come from the central region of the quasar  not jets  and jets are not expected at the accretion rates estimated for the BH in CR7 because they have only been observed at L   0 01 LEdd and L   LEdd  Finally  we note that while the detection of radio emission would conﬁrm the presence of a BH in CR7  the failure to do so would not rule out its existence  It could be that the radio ﬂuxes associated with DCBH candidates lie below those predicted by FPs today so the discovery of a BH in CR7 may have to await future observatories   The authors thank Bhaskar Agarwal  Philip Best  Si  mon Glover and Marta Volonteri for helpful discussions  D  J  W  was supported by STFC New Applicant Grant ST P000509 1 and the Ida Pfeiﬀer Professorship at the Institute of Astrophysics at the University of Vienna  M  M  acknowledges support from the Beatriu de Pinos fel  lowship  2017 BP 00114   A  M  acknowledges support from the UK Science and Technology Facilities Coun  cil Consolidated Grant ST R000972 1  T  H  was sup  ported by JSPS KAKENHI Grant Number 17F17320  M  L  acknowledges funding from UAEU via UPAR grant No  31S372   REFERENCES  Agarwal  B   Johnson  J  L   Zackrisson  E   Labbe  I   van den  Bosch  F  C   Natarajan  P     Khochfar  S  2016  MNRAS Alvarez  M  A   Wise  J  H     Abel  T  2009  ApJ  701  L133 Ba nados  E   et al  2018  Nature  553  473 Bonchi  A   La Franca  F   Melini  G   Bongiorno  A     Fiore  F   2013  MNRAS  429  1970  Machacek  M  E   Bryan  G  L     Abel  T  2003  MNRAS  338   273  Marconi  A   Risaliti  G   Gilli  R   Hunt  L  K   Maiolino  R      Salvati  M  2004  MNRAS  351  169  Matsuoka  Y   et al  2019  ApJ  872  L2 Matthee  J   Sobral  D   Santos  S   R ottgering  H   Darvish  B      Bowler  R  A  A   McLure  R  J   Dunlop  J  S   McLeod  D  J    Mobasher  B  2015  MNRAS  451  400  Stanway  E  R   Eldridge  J  J     Jarvis  M  J  2017  MNRAS  469  448  Bowler  R  A  A   et al  2012  MNRAS  426  2772 Condon  J  J   Cotton  W  D     Broderick  J  J  2002  AJ  124   Matthee  J   et al  2017  ApJ  851  145 Meiksin  A     Whalen  D  J  2013  MNRAS  430  2854 Merloni  A   Heinz  S     di Matteo  T  2003  MNRAS  345  1057 Mezcua  M  2017  International Journal of Modern Physics D  26   675  1730021  Fabian  A  C   Walker  S  A   Celotti  A   Ghisellini  G   Mocz  P    Mezcua  M   Hlavacek Larrondo  J   Lucey  J  R   Hogan  M  T    Blundell  K  M     McMahon  R  G  2014  MNRAS  442  L81  Ghisellini  G   Celotti  A   Tavecchio  F   Haardt  F     Sbarrato   Edge  A  C     McNamara  B  R  2018  MNRAS  474  1342 Mezcua  M   Lobanov  A  P     Mart ı Vidal  I  2013  MNRAS   T  2014  MNRAS  438  2694  G ultekin  K   Cackett  E  M   King  A  L   Miller  J  M      Pinkney  J  2014  ApJ  788  L22  G ultekin  K   Cackett  E  M   Miller  J  M   Di Matteo  T    Markoﬀ  S     Richstone  D  O  2009  ApJ  706  404  G ultekin  K   King  A  L   Cackett  E  M   Nyland  K   Miller   J  M   Di Matteo  T   Markoﬀ  S     Rupen  M  P  2019  ApJ  871  80  Haemmerl e  L   Woods  T  E   Klessen  R  S   Heger  A      Whalen  D  J  2018a  ApJ  853  L3     2018b  MNRAS  474  2757 Hartwig  T   et al  2016  MNRAS  462  2184 Hosokawa  T   Yorke  H  W   Inayoshi  K   Omukai  K      Yoshida  N  2013  ApJ  778  178  Ishigaki  M  N   Tominaga  N   Kobayashi  C     Nomoto  K   2018  ApJ  857  46  Joggerst  C  C   Almgren  A   Bell  J   Heger  A   Whalen  D      Woosley  S  E  2010  ApJ  709  11  K ording  E   Falcke  H     Corbel  S  2006  A A  456  439 La Franca  F   Melini  G     Fiore  F  2010  ApJ  718  368 Latif  M  A     Schleicher  D  R  G  2016  A A  585  A151 Latif  M  A   Schleicher  D  R  G   Schmidt  W     Niemeyer  J   2013  MNRAS  430  588  436  2454  Mortlock  D  J   et al  2011  Nature  474  616 Pacucci  F   Natarajan  P     Ferrara  A  2017a  ApJ  835  L36 Pacucci  F   Pallottini  A   Ferrara  A     Gallerani  S  2017b   MNRAS  468  L77  Pallottini  A   et al  2015  MNRAS  453  2465 Planck Collaboration et al  2016  A A  594  A13 Plotkin  R  M   Markoﬀ  S   Kelly  B  C   K ording  E      Anderson  S  F  2012  MNRAS  419  267  Regan  J  A     Haehnelt  M  G  2009  MNRAS  396  343 Schaerer  D  2002  A A  382  28 Schober  J   Schleicher  D   Federrath  C   Glover  S   Klessen   R  S     Banerjee  R  2012  ApJ  754  99  Shibuya  T   et al  2018  PASJ  70  S15 Smith  B  D   Regan  J  A   Downes  T  P   Norman  M  L    O Shea  B  W     Wise  J  H  2018  MNRAS  480  3762  Smolˇci c  V   et al  2017  A A  602  A1 Sobral  D   Matthee  J   Darvish  B   Schaerer  D   Mobasher  B    R ottgering  H  J  A   Santos  S     Hemmati  S  2015  ApJ  808  139  Whalen  D   Abel  T     Norman  M  L  2004  ApJ  610  14 Whalen  D  J     Fryer  C  L  2012  ApJ  756  L19 Wise  J  H   Turk  M  J     Abel  T  2008  ApJ  682  745   Woods  T  E   Heger  A   Whalen  D  J   Haemmerl e  L      Klessen  R  S  2017  ApJ  842  L6  Woods  T  E   et al  2019  Publications of the Astronomical  Society of Australia  36  e027  5   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x667.491 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Topics found via LDA:\n",
            "\n",
            "Topic #0:\n",
            "bh form collapse luminosity accretion dashed observations detected wise agarwal\n",
            "\n",
            "Topic #1:\n",
            "detection star woods conﬁrm mhz presence bowler obtained grant bon13\n",
            "\n",
            "Topic #2:\n",
            "apj ska large number lr cc kor06 abel cm di\n",
            "\n",
            "Topic #3:\n",
            "al log lx young fp array ultekin explosions institute vlass\n",
            "\n",
            "Topic #4:\n",
            "ghz ﬁrst mass ngvla yr rates section far fig data\n",
            "\n",
            "Topic #5:\n",
            "et whalen sne mezcua limit ambient erg science starburst gul09\n",
            "\n",
            "Topic #6:\n",
            "sn stars pop frame ii quasars njy synchrotron lotss rate\n",
            "\n",
            "Topic #7:\n",
            "cr7 dcbh ﬂux emission µjy remnant matthee black wide beam\n",
            "\n",
            "Topic #8:\n",
            "mnras ﬂuxes university table spectral narrow early ledd relativistic observer\n",
            "\n",
            "Topic #9:\n",
            "radio iii ﬁelds times tokyo high emit upper formation dcbhs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RagmPMdiUYhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U3fSTFqUa1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug09Tfe-UUJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=14WPNDWDkw_Ry8GPdVuyirtb9Is70l2uh'\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('MNBdatatset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jmCRBBgdgFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Copy Paste from here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24hFG0xFUhO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('MNBdatatset.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Q2J_l0U2yq",
        "colab_type": "code",
        "outputId": "75f98a69-55da-45b1-9fec-238fdc5aac09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Word1</th>\n",
              "      <th>Word2</th>\n",
              "      <th>Word3</th>\n",
              "      <th>Word4</th>\n",
              "      <th>Word5</th>\n",
              "      <th>Word6</th>\n",
              "      <th>Word7</th>\n",
              "      <th>Word8</th>\n",
              "      <th>Word9</th>\n",
              "      <th>Word10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fiction script</td>\n",
              "      <td>person</td>\n",
              "      <td>perspective</td>\n",
              "      <td>story</td>\n",
              "      <td>player</td>\n",
              "      <td>fiction</td>\n",
              "      <td>games</td>\n",
              "      <td>character</td>\n",
              "      <td>intent</td>\n",
              "      <td>change</td>\n",
              "      <td>script</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crime story</td>\n",
              "      <td>player</td>\n",
              "      <td>games</td>\n",
              "      <td>story</td>\n",
              "      <td>crime</td>\n",
              "      <td>storytelling</td>\n",
              "      <td>fiction</td>\n",
              "      <td>like</td>\n",
              "      <td>actor</td>\n",
              "      <td>think</td>\n",
              "      <td>mechanical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>game</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>person</td>\n",
              "      <td>end</td>\n",
              "      <td>getting</td>\n",
              "      <td>character</td>\n",
              "      <td>mechanics</td>\n",
              "      <td>investors</td>\n",
              "      <td>game</td>\n",
              "      <td>reader</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comedy movie</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>comedy</td>\n",
              "      <td>don</td>\n",
              "      <td>action</td>\n",
              "      <td>movies</td>\n",
              "      <td>audience</td>\n",
              "      <td>years</td>\n",
              "      <td>good</td>\n",
              "      <td>movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Finance</td>\n",
              "      <td>money</td>\n",
              "      <td>story</td>\n",
              "      <td>going</td>\n",
              "      <td>life</td>\n",
              "      <td>want</td>\n",
              "      <td>banks</td>\n",
              "      <td>new</td>\n",
              "      <td>zero</td>\n",
              "      <td>free</td>\n",
              "      <td>places</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mortgage game</td>\n",
              "      <td>games</td>\n",
              "      <td>genre</td>\n",
              "      <td>person</td>\n",
              "      <td>game</td>\n",
              "      <td>able</td>\n",
              "      <td>mortgage</td>\n",
              "      <td>character</td>\n",
              "      <td>does</td>\n",
              "      <td>just</td>\n",
              "      <td>going</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Big movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>read</td>\n",
              "      <td>book</td>\n",
              "      <td>reason</td>\n",
              "      <td>money</td>\n",
              "      <td>producer</td>\n",
              "      <td>big</td>\n",
              "      <td>people</td>\n",
              "      <td>wasn</td>\n",
              "      <td>just</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>romance movie</td>\n",
              "      <td>person</td>\n",
              "      <td>games</td>\n",
              "      <td>game</td>\n",
              "      <td>genres</td>\n",
              "      <td>narrative</td>\n",
              "      <td>movies</td>\n",
              "      <td>storytelling</td>\n",
              "      <td>11</td>\n",
              "      <td>romance</td>\n",
              "      <td>shooter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Player</td>\n",
              "      <td>movie</td>\n",
              "      <td>player</td>\n",
              "      <td>like</td>\n",
              "      <td>little</td>\n",
              "      <td>point</td>\n",
              "      <td>look</td>\n",
              "      <td>esther</td>\n",
              "      <td>dear</td>\n",
              "      <td>especially</td>\n",
              "      <td>story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>third person game</td>\n",
              "      <td>game</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>fiction</td>\n",
              "      <td>players</td>\n",
              "      <td>literary</td>\n",
              "      <td>based</td>\n",
              "      <td>talk</td>\n",
              "      <td>really</td>\n",
              "      <td>gaming</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Label   Word1        Word2  ...      Word8       Word9      Word10\n",
              "0     fiction script  person  perspective  ...     intent      change      script\n",
              "1        crime story  player        games  ...      actor       think  mechanical\n",
              "2               game   games       player  ...  investors        game      reader\n",
              "3       comedy movie   games       player  ...      years        good       movie\n",
              "4            Finance   money        story  ...       zero        free      places\n",
              "5      Mortgage game   games        genre  ...       does        just       going\n",
              "6          Big movie   movie         read  ...     people        wasn        just\n",
              "7      romance movie  person        games  ...         11     romance     shooter\n",
              "8             Player   movie       player  ...       dear  especially       story\n",
              "9  third person game    game        games  ...       talk      really      gaming\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-otILZRnVnrp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "5d19730a-ed43-4d11-b2a3-748da9863a6a"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim\n",
        "import nltk\n",
        "import gensim\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.38)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.18.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.38)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.7.2)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.0.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (46.1.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.8)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.51.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (2018.9)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMoY78ehWBmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['LDA_Words']=df.Word1 + ' ' + df.Word2 +' ' + df.Word3 +' ' + df.Word4 +' ' + df.Word5 +' ' + df.Word6 +' ' + df.Word7 +' ' + df.Word8 +' ' + df.Word9 +' ' + df.Word10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3dC1CenU3Ny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b5755a05-8ea1-44a5-dd1b-dd23a87abdfe"
      },
      "source": [
        "df"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Word1</th>\n",
              "      <th>Word2</th>\n",
              "      <th>Word3</th>\n",
              "      <th>Word4</th>\n",
              "      <th>Word5</th>\n",
              "      <th>Word6</th>\n",
              "      <th>Word7</th>\n",
              "      <th>Word8</th>\n",
              "      <th>Word9</th>\n",
              "      <th>Word10</th>\n",
              "      <th>LDA_Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fiction script</td>\n",
              "      <td>person</td>\n",
              "      <td>perspective</td>\n",
              "      <td>story</td>\n",
              "      <td>player</td>\n",
              "      <td>fiction</td>\n",
              "      <td>games</td>\n",
              "      <td>character</td>\n",
              "      <td>intent</td>\n",
              "      <td>change</td>\n",
              "      <td>script</td>\n",
              "      <td>person perspective story player fiction games ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crime story</td>\n",
              "      <td>player</td>\n",
              "      <td>games</td>\n",
              "      <td>story</td>\n",
              "      <td>crime</td>\n",
              "      <td>storytelling</td>\n",
              "      <td>fiction</td>\n",
              "      <td>like</td>\n",
              "      <td>actor</td>\n",
              "      <td>think</td>\n",
              "      <td>mechanical</td>\n",
              "      <td>player games story crime storytelling fiction ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>game</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>person</td>\n",
              "      <td>end</td>\n",
              "      <td>getting</td>\n",
              "      <td>character</td>\n",
              "      <td>mechanics</td>\n",
              "      <td>investors</td>\n",
              "      <td>game</td>\n",
              "      <td>reader</td>\n",
              "      <td>games player person end getting character mech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comedy movie</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>comedy</td>\n",
              "      <td>don</td>\n",
              "      <td>action</td>\n",
              "      <td>movies</td>\n",
              "      <td>audience</td>\n",
              "      <td>years</td>\n",
              "      <td>good</td>\n",
              "      <td>movie</td>\n",
              "      <td>games player comedy don action movies audience...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Finance</td>\n",
              "      <td>money</td>\n",
              "      <td>story</td>\n",
              "      <td>going</td>\n",
              "      <td>life</td>\n",
              "      <td>want</td>\n",
              "      <td>banks</td>\n",
              "      <td>new</td>\n",
              "      <td>zero</td>\n",
              "      <td>free</td>\n",
              "      <td>places</td>\n",
              "      <td>money story going life want banks new zero fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mortgage game</td>\n",
              "      <td>games</td>\n",
              "      <td>genre</td>\n",
              "      <td>person</td>\n",
              "      <td>game</td>\n",
              "      <td>able</td>\n",
              "      <td>mortgage</td>\n",
              "      <td>character</td>\n",
              "      <td>does</td>\n",
              "      <td>just</td>\n",
              "      <td>going</td>\n",
              "      <td>games genre person game able mortgage characte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Big movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>read</td>\n",
              "      <td>book</td>\n",
              "      <td>reason</td>\n",
              "      <td>money</td>\n",
              "      <td>producer</td>\n",
              "      <td>big</td>\n",
              "      <td>people</td>\n",
              "      <td>wasn</td>\n",
              "      <td>just</td>\n",
              "      <td>movie read book reason money producer big peop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>romance movie</td>\n",
              "      <td>person</td>\n",
              "      <td>games</td>\n",
              "      <td>game</td>\n",
              "      <td>genres</td>\n",
              "      <td>narrative</td>\n",
              "      <td>movies</td>\n",
              "      <td>storytelling</td>\n",
              "      <td>11</td>\n",
              "      <td>romance</td>\n",
              "      <td>shooter</td>\n",
              "      <td>person games game genres narrative movies stor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Player</td>\n",
              "      <td>movie</td>\n",
              "      <td>player</td>\n",
              "      <td>like</td>\n",
              "      <td>little</td>\n",
              "      <td>point</td>\n",
              "      <td>look</td>\n",
              "      <td>esther</td>\n",
              "      <td>dear</td>\n",
              "      <td>especially</td>\n",
              "      <td>story</td>\n",
              "      <td>movie player like little point look esther dea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>third person game</td>\n",
              "      <td>game</td>\n",
              "      <td>games</td>\n",
              "      <td>player</td>\n",
              "      <td>fiction</td>\n",
              "      <td>players</td>\n",
              "      <td>literary</td>\n",
              "      <td>based</td>\n",
              "      <td>talk</td>\n",
              "      <td>really</td>\n",
              "      <td>gaming</td>\n",
              "      <td>game games player fiction players literary bas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Label  ...                                          LDA_Words\n",
              "0     fiction script  ...  person perspective story player fiction games ...\n",
              "1        crime story  ...  player games story crime storytelling fiction ...\n",
              "2               game  ...  games player person end getting character mech...\n",
              "3       comedy movie  ...  games player comedy don action movies audience...\n",
              "4            Finance  ...  money story going life want banks new zero fre...\n",
              "5      Mortgage game  ...  games genre person game able mortgage characte...\n",
              "6          Big movie  ...  movie read book reason money producer big peop...\n",
              "7      romance movie  ...  person games game genres narrative movies stor...\n",
              "8             Player  ...  movie player like little point look esther dea...\n",
              "9  third person game  ...  game games player fiction players literary bas...\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krcCzKprWoCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[['Label','LDA_Words']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STXDKFIuWtr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "7d486116-5982-4791-9424-4ae457e85197"
      },
      "source": [
        "df"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>LDA_Words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fiction script</td>\n",
              "      <td>person perspective story player fiction games ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>crime story</td>\n",
              "      <td>player games story crime storytelling fiction ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>game</td>\n",
              "      <td>games player person end getting character mech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>comedy movie</td>\n",
              "      <td>games player comedy don action movies audience...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Finance</td>\n",
              "      <td>money story going life want banks new zero fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mortgage game</td>\n",
              "      <td>games genre person game able mortgage characte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Big movie</td>\n",
              "      <td>movie read book reason money producer big peop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>romance movie</td>\n",
              "      <td>person games game genres narrative movies stor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Player</td>\n",
              "      <td>movie player like little point look esther dea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>third person game</td>\n",
              "      <td>game games player fiction players literary bas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Label                                          LDA_Words\n",
              "0     fiction script  person perspective story player fiction games ...\n",
              "1        crime story  player games story crime storytelling fiction ...\n",
              "2               game  games player person end getting character mech...\n",
              "3       comedy movie  games player comedy don action movies audience...\n",
              "4            Finance  money story going life want banks new zero fre...\n",
              "5      Mortgage game  games genre person game able mortgage characte...\n",
              "6          Big movie  movie read book reason money producer big peop...\n",
              "7      romance movie  person games game genres narrative movies stor...\n",
              "8             Player  movie player like little point look esther dea...\n",
              "9  third person game  game games player fiction players literary bas..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k39gQfQZWxlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in df.LDA_Words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWp3f18QXHfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "950b643b-e3ea-4d44-df45-b11638951e7f"
      },
      "source": [
        "gen_docs"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['person',\n",
              "  'perspective',\n",
              "  'story',\n",
              "  'player',\n",
              "  'fiction',\n",
              "  'games',\n",
              "  'character',\n",
              "  'intent',\n",
              "  'change',\n",
              "  'script'],\n",
              " ['player',\n",
              "  'games',\n",
              "  'story',\n",
              "  'crime',\n",
              "  'storytelling',\n",
              "  'fiction',\n",
              "  'like',\n",
              "  'actor',\n",
              "  'think',\n",
              "  'mechanical'],\n",
              " ['games',\n",
              "  'player',\n",
              "  'person',\n",
              "  'end',\n",
              "  'getting',\n",
              "  'character',\n",
              "  'mechanics',\n",
              "  'investors',\n",
              "  'game',\n",
              "  'reader'],\n",
              " ['games',\n",
              "  'player',\n",
              "  'comedy',\n",
              "  'don',\n",
              "  'action',\n",
              "  'movies',\n",
              "  'audience',\n",
              "  'years',\n",
              "  'good',\n",
              "  'movie'],\n",
              " ['money',\n",
              "  'story',\n",
              "  'going',\n",
              "  'life',\n",
              "  'want',\n",
              "  'banks',\n",
              "  'new',\n",
              "  'zero',\n",
              "  'free',\n",
              "  'places'],\n",
              " ['games',\n",
              "  'genre',\n",
              "  'person',\n",
              "  'game',\n",
              "  'able',\n",
              "  'mortgage',\n",
              "  'character',\n",
              "  'does',\n",
              "  'just',\n",
              "  'going'],\n",
              " ['movie',\n",
              "  'read',\n",
              "  'book',\n",
              "  'reason',\n",
              "  'money',\n",
              "  'producer',\n",
              "  'big',\n",
              "  'people',\n",
              "  'wasn',\n",
              "  'just'],\n",
              " ['person',\n",
              "  'games',\n",
              "  'game',\n",
              "  'genres',\n",
              "  'narrative',\n",
              "  'movies',\n",
              "  'storytelling',\n",
              "  '11',\n",
              "  'romance',\n",
              "  'shooter'],\n",
              " ['movie',\n",
              "  'player',\n",
              "  'like',\n",
              "  'little',\n",
              "  'point',\n",
              "  'look',\n",
              "  'esther',\n",
              "  'dear',\n",
              "  'especially',\n",
              "  'story'],\n",
              " ['game',\n",
              "  'games',\n",
              "  'player',\n",
              "  'fiction',\n",
              "  'players',\n",
              "  'literary',\n",
              "  'based',\n",
              "  'talk',\n",
              "  'really',\n",
              "  'gaming']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zp45YaEXL4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_t6yTnHXjAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a40e644-3f9c-4a72-f565-ac25938e69cc"
      },
      "source": [
        "print(dictionary.token2id)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'change': 0, 'character': 1, 'fiction': 2, 'games': 3, 'intent': 4, 'person': 5, 'perspective': 6, 'player': 7, 'script': 8, 'story': 9, 'actor': 10, 'crime': 11, 'like': 12, 'mechanical': 13, 'storytelling': 14, 'think': 15, 'end': 16, 'game': 17, 'getting': 18, 'investors': 19, 'mechanics': 20, 'reader': 21, 'action': 22, 'audience': 23, 'comedy': 24, 'don': 25, 'good': 26, 'movie': 27, 'movies': 28, 'years': 29, 'banks': 30, 'free': 31, 'going': 32, 'life': 33, 'money': 34, 'new': 35, 'places': 36, 'want': 37, 'zero': 38, 'able': 39, 'does': 40, 'genre': 41, 'just': 42, 'mortgage': 43, 'big': 44, 'book': 45, 'people': 46, 'producer': 47, 'read': 48, 'reason': 49, 'wasn': 50, '11': 51, 'genres': 52, 'narrative': 53, 'romance': 54, 'shooter': 55, 'dear': 56, 'especially': 57, 'esther': 58, 'little': 59, 'look': 60, 'point': 61, 'based': 62, 'gaming': 63, 'literary': 64, 'players': 65, 'really': 66, 'talk': 67}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdCO6cMrXpMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mmzYFpPXzJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "307e2816-3ce2-4c42-b980-21b5b0107c7b"
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['change', 0.45], ['character', 0.24], ['fiction', 0.24], ['games', 0.07], ['intent', 0.45], ['person', 0.18], ['perspective', 0.45], ['player', 0.1], ['script', 0.45], ['story', 0.18]]\n",
            "[['fiction', 0.22], ['games', 0.07], ['player', 0.09], ['story', 0.17], ['actor', 0.43], ['crime', 0.43], ['like', 0.3], ['mechanical', 0.43], ['storytelling', 0.3], ['think', 0.43]]\n",
            "[['character', 0.22], ['games', 0.07], ['person', 0.17], ['player', 0.09], ['end', 0.42], ['game', 0.17], ['getting', 0.42], ['investors', 0.42], ['mechanics', 0.42], ['reader', 0.42]]\n",
            "[['games', 0.06], ['player', 0.08], ['action', 0.38], ['audience', 0.38], ['comedy', 0.38], ['don', 0.38], ['good', 0.38], ['movie', 0.2], ['movies', 0.27], ['years', 0.38]]\n",
            "[['story', 0.14], ['banks', 0.35], ['free', 0.35], ['going', 0.25], ['life', 0.35], ['money', 0.25], ['new', 0.35], ['places', 0.35], ['want', 0.35], ['zero', 0.35]]\n",
            "[['character', 0.22], ['games', 0.07], ['person', 0.17], ['game', 0.17], ['going', 0.3], ['able', 0.42], ['does', 0.42], ['genre', 0.42], ['just', 0.3], ['mortgage', 0.42]]\n",
            "[['movie', 0.18], ['money', 0.24], ['just', 0.24], ['big', 0.35], ['book', 0.35], ['people', 0.35], ['producer', 0.35], ['read', 0.35], ['reason', 0.35], ['wasn', 0.35]]\n",
            "[['games', 0.06], ['person', 0.16], ['storytelling', 0.28], ['game', 0.16], ['movies', 0.28], ['11', 0.4], ['genres', 0.4], ['narrative', 0.4], ['romance', 0.4], ['shooter', 0.4]]\n",
            "[['player', 0.08], ['story', 0.15], ['like', 0.26], ['movie', 0.2], ['dear', 0.38], ['especially', 0.38], ['esther', 0.38], ['little', 0.38], ['look', 0.38], ['point', 0.38]]\n",
            "[['fiction', 0.21], ['games', 0.06], ['player', 0.09], ['game', 0.16], ['based', 0.39], ['gaming', 0.39], ['literary', 0.39], ['players', 0.39], ['really', 0.39], ['talk', 0.39]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVdXvCu9X3qH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "99122137-8a7e-4376-c045-db59bb1afa66"
      },
      "source": [
        "sims = gensim.similarities.Similarity('.',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:718: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuVjm7GYV5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2_docs = []\n",
        "\n",
        "file1=['i am awsome, and i have a huge story to tell','i love playing games','dear son, i miss money']\n",
        "file2=['i like to play cricket, because it is very intense game']\n",
        "file3=['i like money, i breathe money, only money']\n",
        "file=[file1,file2,file3]\n",
        "def checkSimilarity(sentences):\n",
        "  for sentence in sentences:\n",
        "    tokens = sent_tokenize(sentence)\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "  print(\"Number of documents:\",len(file2_docs))  \n",
        "  for line in file2_docs:\n",
        "      query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "      query_doc_bow = dictionary.doc2bow(query_doc) \n",
        "  query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "  print('Comparing Result:', sims[query_doc_tf_idf]) \n",
        "  for i,j in enumerate(sims[query_doc_tf_idf]):\n",
        "    print('similarity with ',i,'th document is',j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG7pxa35eJix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "881b0e17-ad03-4114-8bc5-4dfd97ca6719"
      },
      "source": [
        "for i in file:\n",
        "  print(i)\n",
        "  checkSimilarity(i)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i am awsome, and i have a huge story to tell', 'i love playing games', 'dear son, i miss money']\n",
            "Number of documents: 3\n",
            "Comparing Result: [0.         0.         0.         0.         0.14039208 0.\n",
            " 0.13940983 0.         0.31046674 0.        ]\n",
            "similarity with  0 th document is 0.0\n",
            "similarity with  1 th document is 0.0\n",
            "similarity with  2 th document is 0.0\n",
            "similarity with  3 th document is 0.0\n",
            "similarity with  4 th document is 0.14039208\n",
            "similarity with  5 th document is 0.0\n",
            "similarity with  6 th document is 0.13940983\n",
            "similarity with  7 th document is 0.0\n",
            "similarity with  8 th document is 0.31046674\n",
            "similarity with  9 th document is 0.0\n",
            "['i like to play cricket, because it is very intense game']\n",
            "Number of documents: 4\n",
            "Comparing Result: [0.         0.2594301  0.08273242 0.         0.         0.08326412\n",
            " 0.         0.07832992 0.23008668 0.07719494]\n",
            "similarity with  0 th document is 0.0\n",
            "similarity with  1 th document is 0.2594301\n",
            "similarity with  2 th document is 0.08273242\n",
            "similarity with  3 th document is 0.0\n",
            "similarity with  4 th document is 0.0\n",
            "similarity with  5 th document is 0.08326412\n",
            "similarity with  6 th document is 0.0\n",
            "similarity with  7 th document is 0.07832992\n",
            "similarity with  8 th document is 0.23008668\n",
            "similarity with  9 th document is 0.077194944\n",
            "['i like money, i breathe money, only money']\n",
            "Number of documents: 5\n",
            "Comparing Result: [0.         0.09440294 0.         0.         0.23248148 0.\n",
            " 0.23085491 0.         0.08372529 0.        ]\n",
            "similarity with  0 th document is 0.0\n",
            "similarity with  1 th document is 0.09440294\n",
            "similarity with  2 th document is 0.0\n",
            "similarity with  3 th document is 0.0\n",
            "similarity with  4 th document is 0.23248148\n",
            "similarity with  5 th document is 0.0\n",
            "similarity with  6 th document is 0.23085491\n",
            "similarity with  7 th document is 0.0\n",
            "similarity with  8 th document is 0.08372529\n",
            "similarity with  9 th document is 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IueyU3SWeTPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}